
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Network Models &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multi-Network Models" href="multi-network-models.html" />
    <link rel="prev" title="Why Use Statistical Models?" href="why-use-models.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../coverpage.html">
   Hands-on Network Machine Learning with Scikit-Learn and Graspologic
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   1. Preface
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-networks.html">
     1.4. Types of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.5. Types of Network Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/main-challenges.html">
     1.6. Main Challenges of Network Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/exercises.html">
     1.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/transformation-techniques.html">
     2.4. Transformation Techniques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.5. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.6. Fine-Tune your Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/ch4.html">
   1. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/matrix-representations.html">
     1.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/network-representations.html">
     1.2. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/properties-of-networks.html">
     1.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/regularization.html">
     1.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch5.html">
   2. Why Use Statistical Models?
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="why-use-models.html">
     Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi-network-models.html">
     Multi-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="models-with-covariates.html">
     Network Models with Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch6/ch6.html">
   3. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters.html">
     3.1. Estimating Parameters in Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/why-embed-networks.html">
     3.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/random-walk-diffusion-methods.html">
     3.3. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/graph-neural-networks.html">
     3.4. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/multigraph-representation-learning.html">
     3.5. Multigraph Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/joint-representation-learning.html">
     3.6. Joint Representation Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch7/ch7.html">
   4. Theoretical Results
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-single-network.html">
     4.1. Theory for Single Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-multigraph.html">
     4.2. Theory for Multiple-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-matching.html">
     4.3. Theory for Graph Matching
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   1. Leveraging Representations for Single Graph Applications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/community-detection.html">
     1.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/testing-differences.html">
     1.2. Testing for Differences between Communities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/model-selection.html">
     1.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/vertex-nomination.html">
     1.4. Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/anomaly-detection.html">
     1.5. Anomaly Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/out-of-sample.html">
     1.6. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   2. Leveraging Representations for Multiple Graph Applications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/two-sample-hypothesis.html">
     2.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/graph-matching-vertex.html">
     2.2. Graph Matching and Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/vertex-nomination.html">
     2.3. Vertex Nomination
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch10/ch10.html">
   3. Algorithms for more than 2 graphs
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/anomaly-detection.html">
     3.1. Anomaly Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-edges.html">
     3.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-vertices.html">
     3.3. Testing for Significant Vertices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-communities.html">
     3.4. Testing for Significant Communities
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/representations/ch5/single-network-models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/representations/ch5/single-network-models.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/representations/ch5/single-network-models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Network Models
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#foundation">
     *Foundation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#equivalence-classes">
       *Equivalence Classes
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erdos-renyi-er">
     Erdös-Rényi (ER)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#practical-utility">
       Practical Utility
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#code-examples">
       Code Examples
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#likelihood">
       *Likelihood
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-block-model-sbm">
     Stochastic Block Model (SBM)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-priori-stochastic-block-model">
       <em>
        A Priori
       </em>
       Stochastic Block Model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Code Examples
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id2">
         *Likelihood
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-posteriori-stochastic-block-model">
       <em>
        A Posteriori
       </em>
       Stochastic Block Model
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id3">
         *Likelihood
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-dot-product-graph-rdpg">
     Random Dot Product Graph (RDPG)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-priori-rdpg">
       <em>
        A Priori
       </em>
       RDPG
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id4">
         Code Examples
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       *Likelihood
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-posteriori-rdpg">
       <em>
        A Posteriori
       </em>
       RDPG
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id6">
         *Likelihood
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inhomogeneous-erdos-renyi-ier">
     Inhomogeneous Erdös-Rényi (IER)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id7">
       *Likelihood
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#degree-corrected-stochastic-block-model-dcsbm">
     Degree-Corrected Stochastic Block Model (DCSBM)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-priori-dcsbm">
       <em>
        A Priori
       </em>
       DCSBM
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#network-models-for-networks-which-aren-t-simple">
     Network Models for Networks which aren’t Simple
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#binary-network-model-which-has-loops-but-is-undirected">
       Binary Network Model which has Loops, but is Undirected
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#binary-network-model-which-is-loopless-but-directed">
       Binary Network Model which is Loopless, but Directed
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#binary-network-model-which-is-has-loops-and-is-directed">
       Binary Network Model which is has Loops and is Directed
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-random-dot-product-graph-grdpg">
     Generalized Random Dot Product Graph (GRDPG)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="network-models">
<h1>Network Models<a class="headerlink" href="#network-models" title="Permalink to this headline">¶</a></h1>
<p>Probably the easiest kinds of statistical models for us to think about are the <em>network models</em>. These types of models (like the name imples) describe the random processes which you’d find when you’re only looking at one network. We can have models which assume all of the nodes connect to each other essentially randomly, models which assume that the nodes are in distinct <em>communities</em>, and many more.</p>
<p>The important realization to make about statistical models is that a model is <em>not</em> a network: it’s the random process that <em>creates</em> a network. You can sample from a model a bunch of times, and because it’s a random process, you’ll end up with networks that look a little bit different each time – but if you sampled a lot of networks and then averaged them, then you’d likely be able to get a reasonable ballpark estimation of what the model that they come from looks like.</p>
<p>Let’s pretend that we have a network, and the network is unweighted (meaning, we only have edges or not-edges) and undirected (meaning, edges connect nodes both ways). It’d have an adjacency matrix which consists of only 1’s and 0’s, because the only information we care about is whether there’s an edge or not. The model that generated this network is pretty straightforward: there’s just some universal probability that each node connects to each other node, and there are 10 nodes.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">binary_heatmap</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">p</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
<span class="n">binary_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;A small, simple network&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_1_0.png" src="../../_images/single-network-models_1_0.png" />
</div>
</div>
<p>This small, simple network is one of many possible networks that we can generate with this model. Here are some more:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="o">.</span><span class="n">flat</span><span class="p">:</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="n">hmap</span> <span class="o">=</span> <span class="n">binary_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">yticklabels</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">linecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Three small, simple networks&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_3_0.png" src="../../_images/single-network-models_3_0.png" />
</div>
</div>
<p>One reasonable question to ask is how <em>many</em> possible networks we could make in this simple scenario? We’ve already made four, and it seems like there are more that this model could potentially generate.</p>
<p>As it turns out, “more” is a pretty massive understatement. To actually figure out the number, think about the first node: there are two possibilities (weighted or not weighted), so you can generate two networks from a one-node model. Now, let’s add an additional node. For each of the first two possibilities, there are two more – so there are <span class="math notranslate nohighlight">\(2 \times 2 = 4\)</span> total possible networks. Every node that we add doubles the number of networks - and since a network with <span class="math notranslate nohighlight">\(n\)</span> nodes has <span class="math notranslate nohighlight">\(n \times n\)</span> edges, the total number of possible networks ends up being <span class="math notranslate nohighlight">\(2^{n \times n} = 2^{n^2}\)</span>! So this ten-node model can generate <span class="math notranslate nohighlight">\(2^{10^2} = 2^{100}\)</span> networks, which is, when you think carefully, an absurdly, ridiculously big number.</p>
<p>Throughout many of the succeeding sections, we will attempt to make the content accessible to readers with, and without, a more technical background. To this end, we have added sections with leading asterisks (*). While we believe these sections build technical depth, we don’t think they are critical to understanding many of the core ideas for network machine learning. In contrast with unstarred sections, these sections will assume familiarity with more advanced mathematical and probability concepts.</p>
<div class="section" id="foundation">
<h2>*Foundation<a class="headerlink" href="#foundation" title="Permalink to this headline">¶</a></h2>
<p>To understand network models, it is crucial to understand the concept of a network as a random quantity, taking a probability distribution. We have a realization <span class="math notranslate nohighlight">\(A\)</span>, and we think that this realization is random in some way. Stated another way, we think that there exists a network-valued random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> that governs the realizations we get to see. Since <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random variable, we can describe it using a probability distribution. The distribution of the network topology is the function <span class="math notranslate nohighlight">\(\mathbb P\)</span> which, if <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an unweighted random network with <span class="math notranslate nohighlight">\(n\)</span> edges, assigns probabilities to every possible configuration that <span class="math notranslate nohighlight">\(\mathbf A\)</span> could take. Notationally, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P\)</span>, which is read in words as “the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is distributed according to <span class="math notranslate nohighlight">\(\mathbb P\)</span>.”</p>
<p>In the preceding description, we made a fairly substantial claim: <span class="math notranslate nohighlight">\(\mathbb P\)</span> assigns probabilities to every possible configuration that realizations of <span class="math notranslate nohighlight">\(\mathbf A\)</span>, denoted by <span class="math notranslate nohighlight">\(A\)</span>, could take. How many possibilities are there for a network with <span class="math notranslate nohighlight">\(n\)</span> nodes? Let’s limit ourselves to simple networks: that is, <span class="math notranslate nohighlight">\(A\)</span> takes values that are unweighted (<span class="math notranslate nohighlight">\(A\)</span> is <em>binary</em>), undirected (<span class="math notranslate nohighlight">\(A\)</span> is <em>symmetric</em>), and loopless (<span class="math notranslate nohighlight">\(A\)</span> is <em>hollow</em>). Formally, we describe <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal A_n \triangleq \left\{A : A \textrm{ is an $n \times n$ matrix with $0$s and $1$s}, A\textrm{ is symmetric}, A\textrm{ is hollow}\right\}
\end{align*}\]</div>
<p>In words, <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> is the set of all possible adjacency matrices <span class="math notranslate nohighlight">\(A\)</span> that correspond to simple networks with <span class="math notranslate nohighlight">\(n\)</span> nodes. Stated another way: every <span class="math notranslate nohighlight">\(A\)</span> that is found in <span class="math notranslate nohighlight">\(\mathcal A\)</span> is a <em>binary</em> <span class="math notranslate nohighlight">\(n \times n\)</span> matrix (<span class="math notranslate nohighlight">\(A \in \{0, 1\}^{n \times n}\)</span>), <span class="math notranslate nohighlight">\(A\)</span> is symmetric (<span class="math notranslate nohighlight">\(A = A^\top\)</span>), and <span class="math notranslate nohighlight">\(A\)</span> is <em>hollow</em> (<span class="math notranslate nohighlight">\(diag(A) = 0\)</span>, or <span class="math notranslate nohighlight">\(A_{ii} = 0\)</span> for all <span class="math notranslate nohighlight">\(i = 1,...,n\)</span>). To summarize the statement that <span class="math notranslate nohighlight">\(\mathbb P\)</span> assigns probabilities to every possible configuration that realizations of <span class="math notranslate nohighlight">\(\mathbf A\)</span> can take, we write that <span class="math notranslate nohighlight">\(\mathbb P : \mathcal A_n \rightarrow [0, 1]\)</span>. This means that for any <span class="math notranslate nohighlight">\(A \in \mathcal A_n\)</span> which is a possible realization of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>, that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf A = A)\)</span> is a probability (it takes a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>). If it is completely unambiguous what the random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> refers to, we might abbreviate <span class="math notranslate nohighlight">\(\mathbb P(\mathbf A = A)\)</span> with <span class="math notranslate nohighlight">\(\mathbb P(A)\)</span>. This statement can alternatively be read that the probability that the random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> takes the value <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(\mathbb P(A)\)</span>. Finally, let’s address that question we had in the previous paragraph. How many possible adjacency matrices are in <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>?</p>
<p>Let’s imagine what just one <span class="math notranslate nohighlight">\(A \in \mathcal A_n\)</span> can look like. Note that each matrix <span class="math notranslate nohighlight">\(A\)</span> has <span class="math notranslate nohighlight">\(n \times n = n^2\)</span> possible entries, in total, since it is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix. As there are no diagonal entries to our matrix <span class="math notranslate nohighlight">\(A\)</span>, this means that there are <span class="math notranslate nohighlight">\(n^2 - n = n(n - 1)\)</span> values that are definitely not <span class="math notranslate nohighlight">\(0\)</span> in <span class="math notranslate nohighlight">\(A\)</span>, since there are <span class="math notranslate nohighlight">\(n\)</span> diagonal entries of <span class="math notranslate nohighlight">\(A\)</span> (due to the fact that it is <em>hollow</em>).  As the transposes are the same (because of the <em>symmetry</em> of <span class="math notranslate nohighlight">\(A\)</span>), for every <span class="math notranslate nohighlight">\(a_{ij}\)</span>, we would also <em>double count</em> the <span class="math notranslate nohighlight">\(a_{ji}\)</span> entry. This means that <span class="math notranslate nohighlight">\(A\)</span> has a total of <span class="math notranslate nohighlight">\(\frac{1}{2}n(n - 1)\)</span> possible entries which are <em>free</em>, which is equal to the expression <span class="math notranslate nohighlight">\(\binom{n}{2}\)</span>. Finally, note that for each entry of <span class="math notranslate nohighlight">\(A\)</span>, that the adjacency can take one of <span class="math notranslate nohighlight">\(2\)</span> possible values: <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. To write this down formally, for every possible edge which is randomly determined, we have <em>two</em> possible values that edge could take. Let’s think about building some intuition here:</p>
<ol class="simple">
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(2 \times 2\)</span>, there are <span class="math notranslate nohighlight">\(\binom{2}{2} = 1\)</span> unique entry of <span class="math notranslate nohighlight">\(A\)</span>, which takes one of <span class="math notranslate nohighlight">\(2\)</span> values. There are <span class="math notranslate nohighlight">\(2\)</span> possible ways that <span class="math notranslate nohighlight">\(A\)</span> could look:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \begin{bmatrix}
        0 &amp; 1 \\
        1 &amp; 0
    \end{bmatrix}\textrm{ or }
    \begin{bmatrix}
        0 &amp; 0 \\
        0 &amp; 0
    \end{bmatrix}
\end{align*}\]</div>
<ol class="simple">
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(3 \times 3\)</span>, there are <span class="math notranslate nohighlight">\(\binom{3}{2} = \frac{3 \times 2}{2} = 3\)</span> unique entries of <span class="math notranslate nohighlight">\(A\)</span>, each of which takes one of <span class="math notranslate nohighlight">\(2\)</span> values. There are <span class="math notranslate nohighlight">\(8\)</span> possible ways that <span class="math notranslate nohighlight">\(A\)</span> could look:</p></li>
</ol>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\begin{bmatrix}
    0 &amp; 1 &amp; 1 \\
    1 &amp; 0 &amp; 1 \\
    1 &amp; 1 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 1 &amp; 0 \\
    1 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 1 \\
    0 &amp; 0 &amp; 1 \\
    1 &amp; 1 &amp; 0
    \end{bmatrix}
    \textrm{ or }\\
&amp;\begin{bmatrix}
    0 &amp; 1 &amp; 1 \\
    1 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 1 \\
    0 &amp; 0 &amp; 0 \\
    1 &amp; 0 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 \\
    0 &amp; 1 &amp; 0
    \end{bmatrix}\textrm{ or }\\
&amp;\begin{bmatrix}
    0 &amp; 1 &amp; 0 \\
    1 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0
    \end{bmatrix}\textrm{ or }
\begin{bmatrix}
    0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0
    \end{bmatrix}
\end{align*}\]</div>
<p>How do we generalize this to an arbitrary choice of <span class="math notranslate nohighlight">\(n\)</span>? The answer is to use something called <em>combinatorics</em>. Basically, the approach is to look at each entry of <span class="math notranslate nohighlight">\(A\)</span> which can take different values, and multiply the total number of possibilities by <span class="math notranslate nohighlight">\(2\)</span> for every element which can take different values. Stated another way, if there are <span class="math notranslate nohighlight">\(2\)</span> choices for each one of <span class="math notranslate nohighlight">\(x\)</span> possible items, we have <span class="math notranslate nohighlight">\(2^x\)</span> possible ways in which we could select those <span class="math notranslate nohighlight">\(x\)</span> items. But we already know how many different elements there are in <span class="math notranslate nohighlight">\(A\)</span>, so we are ready to come up with an expression for the number. In total, there are <span class="math notranslate nohighlight">\(2^{\binom n 2}\)</span> unique adjacency matrices in <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>. Stated another way, the <em>cardinality</em> of <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>, described by the expression <span class="math notranslate nohighlight">\(|\mathcal A_n|\)</span>, is <span class="math notranslate nohighlight">\(2^{\binom n 2}\)</span>. When <span class="math notranslate nohighlight">\(n\)</span> is just <span class="math notranslate nohighlight">\(15\)</span>, note that <span class="math notranslate nohighlight">\(\left|\mathcal A_{15}\right| = 2^{\binom{15}{2}} = 2^{105}\)</span>, which when expressed as a power of <span class="math notranslate nohighlight">\(10\)</span>, is more than <span class="math notranslate nohighlight">\(10^{30}\)</span> possible networks that can be realized with just <span class="math notranslate nohighlight">\(15\)</span> nodes! As <span class="math notranslate nohighlight">\(n\)</span> increases, how many unique possible networks are there? we look at the value of <span class="math notranslate nohighlight">\(|\mathcal A_n| = 2^{\binom n 2}\)</span> as a function of <span class="math notranslate nohighlight">\(n\)</span>. As we can see, as <span class="math notranslate nohighlight">\(n\)</span> gets big, <span class="math notranslate nohighlight">\(|\mathcal A_n|\)</span> grows really really fast!</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">comb</span>


<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span>
<span class="n">logAn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">comb</span><span class="p">(</span><span class="n">ni</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">ni</span> <span class="ow">in</span> <span class="n">n</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">logAn</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of Nodes&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Number of Possible Graphs $|A_n|$ (log scale)&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">350</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;$10^{{</span><span class="si">{pow:d}</span><span class="s2">}}$&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">pow</span><span class="o">=</span><span class="n">d</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">250</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">350</span><span class="p">]])</span>
<span class="n">ax</span><span class="p">;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_7_0.png" src="../../_images/single-network-models_7_0.png" />
</div>
</div>
<p>So, now we know that we have probability distributions on networks, and a set <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> which defines all of the adjacency matrices that every probability distribution must assign a probability to. Now, just what is a network model? A <strong>network model</strong> is a set <span class="math notranslate nohighlight">\(\mathcal P\)</span> of probability distributions on <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>. Stated another way, we can describe <span class="math notranslate nohighlight">\(\mathcal P\)</span> to be:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal P &amp;\subseteq \{\mathbb P: \mathbb P\textrm{ is a probability distribution on }\mathcal A_n\}
\end{align*}\]</div>
<p>In general, we will simplify <span class="math notranslate nohighlight">\(\mathcal P\)</span> through something called <em>parametrization</em>. We define <span class="math notranslate nohighlight">\(\Theta\)</span> to be the set of all possible parameters of the random network model, and <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> is a particular parameter choice that governs the parameters of a specific random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>. In this case, we will write <span class="math notranslate nohighlight">\(\mathcal P\)</span> as the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal P(\Theta) &amp;\triangleq \left\{\mathbb P_\theta : \theta \in \Theta\right\}
\end{align*}\]</div>
<p>If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network that follows a network model, we will write that <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P_\theta\)</span>, for some choice <span class="math notranslate nohighlight">\(\theta\)</span>. As above, if it is totally unambiguous what <span class="math notranslate nohighlight">\(\theta\)</span> refers to, we will just use the shorthand <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P\)</span>.</p>
<p>If you are used to traditional univariate or multivariate statistical modelling, an extremely natural choice for when you have a discrete sample space (like <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>, which is discrete because we can count it) would be to use a categorical model. In the categorical model, we would have a single parameter for all possible configurations of an <span class="math notranslate nohighlight">\(n\)</span>-node network; that is, <span class="math notranslate nohighlight">\(|\theta| = \left|\mathcal A_n\right| = 2^{\binom n 2}\)</span>. What is wrong with this model? The limitations are two-fold:</p>
<ol class="simple">
<li><p>As we explained previously, when <span class="math notranslate nohighlight">\(n\)</span> is just <span class="math notranslate nohighlight">\(15\)</span>, we would need over <span class="math notranslate nohighlight">\(10^{30}\)</span> bits of storage just to define <span class="math notranslate nohighlight">\(\theta\)</span>. This amounts to more than <span class="math notranslate nohighlight">\(10^{8}\)</span> zetabytes, which exceeds the storage capacity of <em>the entire world</em>.</p></li>
<li><p>With a single network observed (or really, any number of networks we could collect in the real world) we would never be able to estimate <span class="math notranslate nohighlight">\(2^{\binom n 2}\)</span> parameters for any reasonably non-trivial number of nodes <span class="math notranslate nohighlight">\(n\)</span>. For the case of one observed network <span class="math notranslate nohighlight">\(A\)</span>, an estimate of <span class="math notranslate nohighlight">\(\theta\)</span> (referred to as <span class="math notranslate nohighlight">\(\hat\theta\)</span>) would simply be for <span class="math notranslate nohighlight">\(\hat\theta\)</span> to have a <span class="math notranslate nohighlight">\(1\)</span> in the entry corresponding to our observed network, and a <span class="math notranslate nohighlight">\(0\)</span> everywhere else. Inferentially, this would imply that the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which governs realizations <span class="math notranslate nohighlight">\(A\)</span> is deterministic, even if this is not the case. Even if we collected potentially <em>many</em> observed networks, we would still (with very high probability) just get <span class="math notranslate nohighlight">\(\hat \theta\)</span> as a series of point masses on the observed networks we see, and <span class="math notranslate nohighlight">\(0\)</span>s everywhere else. This would mean our parameter estimates <span class="math notranslate nohighlight">\(\hat\theta\)</span> would not generalize to new observations at <em>all</em>, with high probability.</p></li>
</ol>
<p>So, what are some more reasonable descriptions of <span class="math notranslate nohighlight">\(\mathcal P\)</span>? We explore some choices below. Particularly, we will be most interested in the <em>independent-edge</em> networks. These are the families of networks in which the generative procedure which governs the random networks assume that the edges of the network are generated <em>independently</em>. <strong>Statistical Independence</strong> is a property which greatly simplifies many of the modelling assumptions which are crucial for proper estimation and rigorous statistical inference, which we will learn more about in the later chapters.</p>
<div class="section" id="equivalence-classes">
<h3>*Equivalence Classes<a class="headerlink" href="#equivalence-classes" title="Permalink to this headline">¶</a></h3>
<p>In all of the below models, we will explore the concept of the <strong>likelihood equivalence class</strong>, or an <em>equivalence class</em>, for short. The likelihood <span class="math notranslate nohighlight">\(\mathcal L\)</span> is a function which in general, describes how effective a particular observation can be described by a random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> with parameters <span class="math notranslate nohighlight">\(\theta\)</span>, written <span class="math notranslate nohighlight">\(\mathbf A \sim F(\theta)\)</span>. Formally, the likelihood is the function where <span class="math notranslate nohighlight">\(\mathcal L_\theta(A) \propto \mathbb P_\theta(A)\)</span>; that is, the likelihood is proportional to the probability of observing the realization <span class="math notranslate nohighlight">\(A\)</span> if the underlying random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> has parameters <span class="math notranslate nohighlight">\(\theta\)</span>.  Why does this matter when it comes to equivalence classes? An equivalence class is a subset of the sample space <span class="math notranslate nohighlight">\(E \subseteq \mathcal A_n\)</span>, which has the following properties. Holding the parameters <span class="math notranslate nohighlight">\(\theta\)</span> fixed:</p>
<ol class="simple">
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A'\)</span> are members of the same equivalence class <span class="math notranslate nohighlight">\(E\)</span> (written <span class="math notranslate nohighlight">\(A, A' \in E\)</span>), then <span class="math notranslate nohighlight">\(\mathcal L_\theta(A) = \mathcal L_\theta(A')\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A''\)</span> are members of different equivalence classes; that is, <span class="math notranslate nohighlight">\(A \in E\)</span> and <span class="math notranslate nohighlight">\(A'' \in E'\)</span> where <span class="math notranslate nohighlight">\(E, E'\)</span> are equivalence classes, then <span class="math notranslate nohighlight">\(\mathcal L_\theta(A) \neq \mathcal L_\theta(A'')\)</span>.</p></li>
<li><p>Using points 1. and 2., we can establish that if <span class="math notranslate nohighlight">\(E\)</span> and <span class="math notranslate nohighlight">\(E'\)</span> are two different equivalence classes, then <span class="math notranslate nohighlight">\(E \cap E' = \varnothing\)</span>.</p></li>
<li><p>We can use the preceding properties to deduce that given the sample space <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> and a likelihood function <span class="math notranslate nohighlight">\(\mathcal L_\theta\)</span>, we can define a partition of the sample space into equivalence classes <span class="math notranslate nohighlight">\(E_i\)</span>, where <span class="math notranslate nohighlight">\(i \in \mathcal I\)</span> is an arbitrary indexing set.</p></li>
</ol>
<p>We will see more below about how the equivalence classes come into play with network models, and in a later section, we will see their relevance to the estimation of the parameters <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
</div>
</div>
<div class="section" id="erdos-renyi-er">
<h2>Erdös-Rényi (ER)<a class="headerlink" href="#erdos-renyi-er" title="Permalink to this headline">¶</a></h2>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># network with 50 nodes</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># probability of an edge existing is .3</span>

<span class="c1"># sample a single simple adj. mtx from ER(50, .3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">binary_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$ER_</span><span class="si">{50}</span><span class="s2">(0.3)$ Simulation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_10_0.png" src="../../_images/single-network-models_10_0.png" />
</div>
</div>
<p>The simplest random network model is called the Erdös Rényi (ER) model<sup>1</sup>. Consider the social network example explained above. The simplest possible thing to do with our network would be to assume that a given pair of people within our network have the same chance of being friends as any other pair of people we select. The Erdös Rényi model formalizes this relatively simple model with a single parameter:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([0, 1]\)</span></p></td>
<td><p>Probability that an edge exists between a pair of nodes</p></td>
</tr>
</tbody>
</table>
<p>In an Erdös Rényi network, each pair of nodes is connected with probability <span class="math notranslate nohighlight">\(p\)</span>, and therefore not connected with probability <span class="math notranslate nohighlight">\(1-p\)</span>. Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <span class="math notranslate nohighlight">\(Bern(p)\)</span> distribution, whenever <span class="math notranslate nohighlight">\(j &gt; i\)</span>. The word “independent” means that edges in the network occurring or not occurring do not affect one another. For instance, this means that if we knew a student named Alice was friends with Bob, and Alice was also friends with Chadwick, that we do not learn any information about whether Bob is friends with Chadwick. The word “identical” means that every edge in the network has the same probability <span class="math notranslate nohighlight">\(p\)</span> of being connected. If Alice and Bob are friends with probability <span class="math notranslate nohighlight">\(p\)</span>, then Alice/Bob and Chadwick are friends with probability <span class="math notranslate nohighlight">\(p\)</span>, too. When <span class="math notranslate nohighlight">\(i &gt; j\)</span>, we allow <span class="math notranslate nohighlight">\(\mathbf a_{ij} = \mathbf a_{ji}\)</span>. This means that the connections <em>across the diagonal</em> of the adjacency matrix are all equal, which means that we have built-in the property of undirectedness into our networks. Also, we let <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>, which means that all self-loops are always unconnected. This means that all the networks are loopless, and the adjacency matrices are hollow. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an ER network with probability <span class="math notranslate nohighlight">\(p\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim ER_n(p)\)</span>.</p>
<div class="section" id="practical-utility">
<h3>Practical Utility<a class="headerlink" href="#practical-utility" title="Permalink to this headline">¶</a></h3>
<p>In practice, the ER model seems like it might be a little too simple to be useful. Why would it ever be useful to think that the best we can do to describe our network is to say that connections exist with some probability? Does this miss a <em>lot</em> of useful questions we might want to answer? Fortunately, there are a number of ways in which the simplicity of the ER model is useful. Given a probability and a number of nodes, we can easily describe the properties we would expect to see in a network if that network were ER. For instance, we know what the degree distribution of an ER network should look like. We can reverse this idea, too: given a network we think might <em>not</em> be ER, we could check whether it’s different in some way from a network which is ER. For instance, if we see a half of the nodes have a very high degree, and the rest of the nodes with a much lower degree, we can reasonably conclude the network might be more complex than can be described by the ER model. If this is the case, we might look for other, more complex, models that could describe our network.</p>
<div class="admonition-working-out-the-expected-degree-in-an-erd-ouml-s-r-eacute-nyi-network admonition">
<p class="admonition-title">Working Out the Expected Degree in an Erdös-Rényi Network</p>
<p>Suppose that <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a simple network which is random. The network has <span class="math notranslate nohighlight">\(n\)</span> nodes <span class="math notranslate nohighlight">\(\mathcal V = (v_i)_{i = 1}^n\)</span>. Recall that the in a simple network, the node degree is <span class="math notranslate nohighlight">\(deg(v_i) = \sum_{j = 1}^n \mathbf a_{ij}\)</span>. What is the expected degree of a node <span class="math notranslate nohighlight">\(v_i\)</span> of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is Erdös-Rényi?</p>
<p>To describe this, we will compute the expectated value of the degree <span class="math notranslate nohighlight">\(deg(v_i)\)</span>, written <span class="math notranslate nohighlight">\(\mathbb E\left[deg(v_i)\right]\)</span>. Let’s see what happens:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i)\right] &amp;= \mathbb E\left[\sum_{j = 1}^n \mathbf a_{ij}\right] \\
    &amp;= \sum_{j = 1}^n \mathbb E[\mathbf a_{ij}]
\end{align*}\]</div>
<p>We use the <em>linearity of expectation</em> in the line above, which means that the expectation of a sum with a finite number of terms being summed over (<span class="math notranslate nohighlight">\(n\)</span>, in this case) is the sum of the expectations. Finally, by definition, all of the edges <span class="math notranslate nohighlight">\(A_{ij}\)</span> have the same distribution: <span class="math notranslate nohighlight">\(Bern(p)\)</span>. The expected value of a random quantity which takes a Bernoulli distribution is just the probability <span class="math notranslate nohighlight">\(p\)</span>. This means every term <span class="math notranslate nohighlight">\(\mathbb E[\mathbf a_{ij}] = p\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i)\right] &amp;= \sum_{j = 1}^n p = n\cdot p
\end{align*}\]</div>
<p>Since all of the <span class="math notranslate nohighlight">\(n\)</span> terms being summed have the same expected value. This holds for <em>every</em> node <span class="math notranslate nohighlight">\(v_i\)</span>, which means that the expected degree of all nodes is an undirected ER network is the same, <span class="math notranslate nohighlight">\(n \cdot p\)</span>.</p>
</div>
<!-- The ER model is also useful for the development of new computational techniques to use on random networks. This is because even if the "best" model for a network is something much more complex, we can still calculate an edge probability $p$ for the network without needing any information but the adjacency matrix. Consider, for instance, a case where we design a new algorithm for a social network, and we want to know how much more RAM we might need as the social network grows. We might want to investigate how the algorithm scales to networks with different numbers of people and different connection probabilities that might be realistic as our social network expands in popularity. Examining how the algorithm operates on ER networks with different values of $n$ and $p$ might be helpful. This is an especially common approach when people deal with networks that are said to be *sparse*. A **sparse network** is a network in which the number of edges is much less than the total possible number of edges. This contrasts with a **dense network**, which is a network in which the number of edges is close to the maximum number of possible edges. In the case of an $ER_{n}(p)$ network, the network is sparse when $p$ is small (closer to $0$), and dense when $p$ is large (closer to $1$). -->
</div>
<div class="section" id="code-examples">
<h3>Code Examples<a class="headerlink" href="#code-examples" title="Permalink to this headline">¶</a></h3>
<p>In the next code block, we look to sample a single ER network with <span class="math notranslate nohighlight">\(50\)</span> nodes and an edge probability <span class="math notranslate nohighlight">\(p\)</span> of <span class="math notranslate nohighlight">\(0.3\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># network with 50 nodes</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># probability of an edge existing is .3</span>

<span class="c1"># sample a single simple adjacency matrix from ER(50, .3)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">binary_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$ER_</span><span class="si">{50}</span><span class="s2">(0.3)$ Simulation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_13_0.png" src="../../_images/single-network-models_13_0.png" />
</div>
</div>
<p>Above, we visualize the network using a heatmap. The dark red squares indicate that an edge exists between a pair of nodes, and white squares indicate that an edge does not exist between a pair of nodes.</p>
<p>Next, let’s see what happens when we use a higher edge probability, like <span class="math notranslate nohighlight">\(p=0.7\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c1"># network has an edge probability of 0.7</span>

<span class="c1"># sample a single adj. mtx from ER(50, 0.7)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">binary_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$ER_</span><span class="si">{50}</span><span class="s2">(0.7)$ Simulation&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_16_0.png" src="../../_images/single-network-models_16_0.png" />
</div>
</div>
<p>As the edge probability increases, the sampled adjacency matrix tends to indicate that there are more connections in the network. This is because there is a higher chance of an edge existing when <span class="math notranslate nohighlight">\(p\)</span> is larger.</p>
</div>
<div class="section" id="likelihood">
<h3>*Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this headline">¶</a></h3>
<p>What is the likelihood for realizations of Erdös-Rényi networks? Remember that the likelihood is proportional to the probability of observing a particular realization <span class="math notranslate nohighlight">\(A\)</span> of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> given the parameters <span class="math notranslate nohighlight">\(\theta\)</span>. With <span class="math notranslate nohighlight">\(\theta = p\)</span> (that is, an ER-random network has only a single parameter, the probability <span class="math notranslate nohighlight">\(p\)</span>) we get:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal L_{\theta}(A) &amp;\propto \mathbb P_{\theta}(\mathbf A = A) \\
    &amp;= \mathbb P_{\theta}(\mathbf a_{11} = a_{11}, ..., \mathbf a_{1n} = a_{1n}, \mathbf a_{23} = a_{23}, ..., \mathbf a_{nn} = a_{nn}) \\
    &amp;= \mathbb P_\theta(\mathbf a_{ij} = a_{ij} \;\forall j &gt; i)
\end{align*}\]</div>
<p>Remember the fact that the entries of <span class="math notranslate nohighlight">\(\mathbf A\)</span> are independent, which means that the probability that the network-valued random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> takes the value <span class="math notranslate nohighlight">\(A\)</span> is the product of the probabilities of the individual random adjacencies <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> taking the values <span class="math notranslate nohighlight">\(a_{ij}\)</span>. Throughout the succeeding sections, we will call this the “Independence Assumption”:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(\mathbf a_{ij} = a_{ij} \;\forall j &gt; i) &amp;= \prod_{j &gt; i} \mathbb P_\theta(\mathbf{a}_{ij} = a_{ij})
\end{align*}\]</div>
<p>Next, we recall that by assumption of the ER model, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij} \sim Bern(p)\)</span> for all <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>, by the identical distribution assumption. Therefore, each <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> takes the probability given by the probability mass function of a Bernoulli random variable, which is <span class="math notranslate nohighlight">\(\mathbb P(\mathbf{a}_{ij} = a_{ij}; p) = p^{a_{ij}}(1 - p)^{1 - a_{ij}}\)</span>. Finally, let <span class="math notranslate nohighlight">\(m\)</span> denote the number of edges present in the network, <span class="math notranslate nohighlight">\(\sum_{j &gt; i} a_{ij}\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal L_\theta(A) &amp;\propto \prod_{j &gt; i} p^{a_{ij}}(1 - p)^{1 - a_{ij}} \\
    &amp;= p^{\sum_{j &gt; i} a_{ij}} \cdot (1 - p)^{\binom{n}{2} - \sum_{j &gt; i}a_{ij}} \\
    &amp;= p^{m} \cdot (1 - p)^{\binom{n}{2} - m}
\end{align*}\]</div>
<p>This means that the likelihood <span class="math notranslate nohighlight">\(\mathcal L_\theta(A)\)</span> is a function <em>only</em> of the number of edges <span class="math notranslate nohighlight">\(m = \sum_{j &gt; i}a_{ij}\)</span> in the network represented by adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>. The equivalence class on the Erdös-Rényi networks are the sets:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E_{i} &amp;= \left\{A \in \mathcal A_n : m = i\right\}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> index from <span class="math notranslate nohighlight">\(0\)</span> (the minimum number of edges possible) all the way up to <span class="math notranslate nohighlight">\(n^2\)</span> (the maximum number of edges possible). All of the relationships for equivalence classes discussed above apply to the sets <span class="math notranslate nohighlight">\(E_i\)</span>.</p>
</div>
</div>
<div class="section" id="stochastic-block-model-sbm">
<h2>Stochastic Block Model (SBM)<a class="headerlink" href="#stochastic-block-model-sbm" title="Permalink to this headline">¶</a></h2>
<p>Imagine that we are flipping a fair single coin. A <em>fair</em> coin is a coin in which the probability of seeing either a heads or a tails on a coin flip is <span class="math notranslate nohighlight">\(\frac{1}{2}\)</span>. Let’s imagine we flip the coin <span class="math notranslate nohighlight">\(20\)</span> times, and we see <span class="math notranslate nohighlight">\(10\)</span> heads and <span class="math notranslate nohighlight">\(10\)</span> tails.</p>
<p>What would happen if we were to flip <span class="math notranslate nohighlight">\(2\)</span> coins, which had a different probability of seeing heads or tails? Imagine that we flip each coin ten times. The first ten flips are with a fair coin, and we might see an outcome of <span class="math notranslate nohighlight">\(5\)</span> heads and <span class="math notranslate nohighlight">\(5\)</span> tails. On the other hand, the second ten flips are not with a fair coin, but with a coin that has a <span class="math notranslate nohighlight">\(\frac{4}{5}\)</span> probability to land on heads, and a <span class="math notranslate nohighlight">\(\frac{1}{5}\)</span> probability of landing on tails. In the second set of <span class="math notranslate nohighlight">\(10\)</span> flips, we might see an outcome of <span class="math notranslate nohighlight">\(9\)</span> heads and <span class="math notranslate nohighlight">\(1\)</span> tails.</p>
<p>In the first set of twenty coin flips, all of the coin flips are performed with the same coin. Stated another way, we have a single <em>cluster</em>, or a set of coin flips which are similar. On the other hand, in the second set of twenty coin flips, twenty of the coin flips are performed with a fair coin, and ten of the coin flips are performed with a different coin which is not fair. Here, we have two <em>clusters</em> of coin flips, those that occur with the first coin, and those that occur with the second coin. Since the first cluster of coin flips are with a fair coin, we expect that coin flips from the first cluster will not necessarily have an identical number of heads and tails, but at least a similar number of heads and tails. On the other hand, coin flips from the second cluster will tend to have more heads than tails.</p>
<p>What does this example have to do with networks? In the above examples, the two sets of coin flips differ in the number of coins with different probabilities that we use for the example. The first example has only one unique coin, whereas the second example has two unique coins with different probabilities of heads or tails. If we were to assume that the second example had been performed with only a single coin when in reality it was performed with two different coins, we would be unable to capture that the second ten coin flips had a substantially different chance of landing on heads than the first ten coin flips. Just like coin flips can be performed with fundamentally different coins, the nodes of a network could also be fundamentally different. The way in which two nodes differ (or do not differ) sometimes holds value in determining the probability that an edge exists between them.</p>
<p>To generalize this example to a network, let’s imagine that we have <span class="math notranslate nohighlight">\(100\)</span> students, each of whom can go to one of two possible schools: school <span class="math notranslate nohighlight">\(1\)</span> or school <span class="math notranslate nohighlight">\(2\)</span>. Our network has <span class="math notranslate nohighlight">\(100\)</span> nodes, which each node represents a single student. The edges of this network represent whether a pair of students are friends. Logically, if two students go to the same school, it might make sense to say that they have a higher chance of being friends than if they do not go to the same school. If we were to try to characterize this network using an ER network, we would run into a problem very similar to when we tried to capture the two cluster coin flip example with only a single coin. Intuitively, there must be a better way!</p>
<p>The Stochastic Block Model, or SBM, captures this idea through the use of a node-assignment vector, which assigns each of the <span class="math notranslate nohighlight">\(n\)</span> nodes in the network to one of <span class="math notranslate nohighlight">\(K\)</span> communities. A <strong>community</strong> is a group of nodes within the network. In our example case, the communities would represent the schools that students are able to attend in our network. In an SBM, instead of describing all pairs of nodes by a fixed probability like with the ER model, we instead describe properties that hold for edges between <em>pairs of communities</em>.  In this sense, for a given pair of communities, the subgraph of edges that comprise these two communities behaves similar to an ER graph, described previously. There are two types of SBMs: one in which the node-assignment vector is treated as <em>unknown</em> and one in which the node-assignment vector is treated as *known (it is a <em>node attribute</em> for the network).</p>
<div class="section" id="a-priori-stochastic-block-model">
<h3><em>A Priori</em> Stochastic Block Model<a class="headerlink" href="#a-priori-stochastic-block-model" title="Permalink to this headline">¶</a></h3>
<p>The <em>a priori</em> SBM is an SBM in which we know <em>a priori</em> (that is, ahead of time) which nodes are in which node communities. Here, we will use the variable <span class="math notranslate nohighlight">\(K\)</span> to denote the maximum number of communities that nodes could be assigned to. The ordering of the communities does not matter; the community we call <span class="math notranslate nohighlight">\(1\)</span> versus <span class="math notranslate nohighlight">\(2\)</span> versus <span class="math notranslate nohighlight">\(K\)</span> is largely a symbolic distinction (the only thing that matters is that they are <em>different</em>). The <em>a priori</em> SBM and has the following two parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vec\tau\)</span></p></td>
<td><p>{1,…,K}<span class="math notranslate nohighlight">\(^n\)</span></p></td>
<td><p>The community assignment vector for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes to one of <span class="math notranslate nohighlight">\(K\)</span> communities</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{K \times K}\)</span></p></td>
<td><p>The block matrix, which assigns edge probabilities for pairs of communities</p></td>
</tr>
</tbody>
</table>
<p>We write that <span class="math notranslate nohighlight">\(\vec \tau \in \{1, ..., K\}^n\)</span>, which means that <span class="math notranslate nohighlight">\(\vec \tau\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector which takes one of <span class="math notranslate nohighlight">\(K\)</span> possible values. In our social network example, for instance, this vector would reflect the fact that each of the <span class="math notranslate nohighlight">\(n\)</span> students in our network could be attendees at one of <span class="math notranslate nohighlight">\(K\)</span> possible schools. For a single node <span class="math notranslate nohighlight">\(i\)</span> that is in community <span class="math notranslate nohighlight">\(\ell\)</span>, where <span class="math notranslate nohighlight">\(\ell \in \{1, ..., K\}\)</span>, we write that <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span>.</p>
<p>Next, let’s discuss the matrix <span class="math notranslate nohighlight">\(B\)</span>, which is known as the <strong>block matrix</strong> of the SBM. We write down that <span class="math notranslate nohighlight">\(B \in [0, 1]^{K \times K}\)</span>, which means that the block matrix is a matrix with <span class="math notranslate nohighlight">\(K\)</span> rows and <span class="math notranslate nohighlight">\(K\)</span> columns. If we have a pair of nodes and know which of the <span class="math notranslate nohighlight">\(K\)</span> communities each node is from, the block matrix tells us the probability that those two nodes are connected. If our networks are simple, the matrix <span class="math notranslate nohighlight">\(B\)</span> is also symmetric, which means that if <span class="math notranslate nohighlight">\(b_{k, \ell} = p\)</span> where <span class="math notranslate nohighlight">\(p\)</span> is a probability, that <span class="math notranslate nohighlight">\(b_{\ell, k} = p\)</span>, too. The requirement of <span class="math notranslate nohighlight">\(B\)</span> to be symmetric exists <em>only</em> if we are dealing with simple networks, since they are undirected; if we relax the requirement of undirectedness (and allow directed networks) <span class="math notranslate nohighlight">\(B\)</span> no longer need be symmetric.</p>
<p>Finally, let’s think about how to write down the generative model for the <em>a priori</em> SBM. Remember that <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are parameters, which means that when we fit a SBM, we know these ahead of time. Intuitionally what we want to reflect is, if we know that node <span class="math notranslate nohighlight">\(i\)</span> is in community <span class="math notranslate nohighlight">\(\ell\)</span> and node <span class="math notranslate nohighlight">\(j\)</span> is in community <span class="math notranslate nohighlight">\(k\)</span>, that the <span class="math notranslate nohighlight">\((\ell, k)\)</span> entry of the block matrix is the probability that <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are connected. Given that  <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(b_{\ell, k})\)</span> distribution for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>.  If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> SBM network with parameters <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim SBM_n(\vec \tau, B)\)</span>.</p>
</div>
<div class="section" id="id1">
<h3>Code Examples<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>We just covered a lot of intuition that will be critical for understanding many of the later models, so let’s work through an example. Say we have <span class="math notranslate nohighlight">\(300\)</span> students, and we know that each student goes to one of two possible schools. We will begin by thinking about the <em>a priori</em> SBM, since it’s a little more straightforward to generate samples. Remember the <em>a priori</em> SBM is the SBM where we know the node-assignment vector <span class="math notranslate nohighlight">\(\vec \tau\)</span> ahead of time. We don’t really care too much about the ordering of the students for now, so let’s just assume that the first <span class="math notranslate nohighlight">\(150\)</span> students all go to school <span class="math notranslate nohighlight">\(1\)</span>, and the second <span class="math notranslate nohighlight">\(150\)</span> students all go to school <span class="math notranslate nohighlight">\(2\)</span>. Let’s assume that the students from school <span class="math notranslate nohighlight">\(1\)</span> are a little bit more closely knit than the students from school <span class="math notranslate nohighlight">\(2\)</span>, so we’ll say that the probability of two students who both go to school <span class="math notranslate nohighlight">\(1\)</span> being friends is <span class="math notranslate nohighlight">\(0.5\)</span>, and the probability of two students who both go to school <span class="math notranslate nohighlight">\(2\)</span> being friends is <span class="math notranslate nohighlight">\(0.3\)</span>. Finally, let’s assume that if one student goes to school <span class="math notranslate nohighlight">\(1\)</span> and the other student goes to school <span class="math notranslate nohighlight">\(2\)</span>, that the probability that they are friends is <span class="math notranslate nohighlight">\(0.2\)</span>.</p>
<div class="admonition-thought-exercise admonition">
<p class="admonition-title">Thought Exercise</p>
<p>Before you read on, try to think to yourself about what the node-assignment vector <span class="math notranslate nohighlight">\(\vec \tau\)</span> and the block matrix <span class="math notranslate nohighlight">\(B\)</span> look like.</p>
</div>
<p>Next, let’s plot what <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> look like:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="k">def</span> <span class="nf">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Node&quot;</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;skyblue&quot;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">((</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">tau</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([</span><span class="s1">&#39;School 1&#39;</span><span class="p">,</span> <span class="s1">&#39;School 2&#39;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="n">xlab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mf">149.5</span><span class="p">,</span><span class="mf">299.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;150&quot;</span><span class="p">,</span> <span class="s2">&quot;300&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># number of students</span>

<span class="c1"># tau is a column vector of 150 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 300 students are from</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Tau, Node Assignment Vector&quot;</span><span class="p">,</span>
        <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_21_0.png" src="../../_images/single-network-models_21_0.png" />
</div>
</div>
<p>So as we can see, the first <span class="math notranslate nohighlight">\(50\)</span> students are from school <span class="math notranslate nohighlight">\(1\)</span>, and the second <span class="math notranslate nohighlight">\(50\)</span> students are from school <span class="math notranslate nohighlight">\(2\)</span>. Next, let’s look at the block matrix <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 2 communities in total</span>
<span class="c1"># construct the block matrix B as described above</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
<span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
<span class="n">B</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">3</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_block</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">blockname</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">blocktix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span>
               <span class="n">blocklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;School 1&quot;</span><span class="p">,</span> <span class="s2">&quot;School 2&quot;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_block</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_24_0.png" src="../../_images/single-network-models_24_0.png" />
</div>
</div>
<p>As we can see, the matrix <span class="math notranslate nohighlight">\(B\)</span> is a symmetric block matrix, since our network is undirected. Finally, let’s sample a single network from the SBM with parameters <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">adjplot</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># sample a graph from SBM_{300}(tau, B)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">)],</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;School&quot;</span><span class="p">:</span> <span class="n">tau</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)}</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">adjplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">meta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_26_0.png" src="../../_images/single-network-models_26_0.png" />
</div>
</div>
<p>The above network shows students, ordered by the school they are in (school 1 and school 2, respectively). As we can see in the above network, people from school <span class="math notranslate nohighlight">\(1\)</span> are more connected than people from school <span class="math notranslate nohighlight">\(2\)</span>.  We notice this from the fact that there are more connections between people from school <span class="math notranslate nohighlight">\(1\)</span> than from school <span class="math notranslate nohighlight">\(2\)</span>. Also, the connections between people from different schools appear to be a bit <em>more sparse</em> (fewer edges) than connections betwen schools. The above heatmap can be described as <strong>modular</strong>: it has clear communities, which are the nodes that comprise the obvious “squares” in the above adjacency matrix.</p>
<p>Something easy to mistake about the SBM is that the SBM will <em>not always</em> have the obvious modular structure defined above when we look at a heatmap. Rather, this modular structure is <em>only</em> made obvious because the students are ordered according to the school in which they are in. What do you think will happen if we look at the students in a random order? Do you think it will be obvious that the network will have a modular structure?</p>
<p>The answer is: <em>No!</em> Let’s see what happens when we use an reordering, called a <em>permutation</em> of the nodes, to reorder the nodes from the network into a random order:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># generate a permutation of the n nodes</span>
<span class="n">vtx_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">meta</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;School&quot;</span><span class="p">:</span> <span class="n">tau</span><span class="p">[</span><span class="n">vtx_perm</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)}</span>
<span class="p">)</span>

<span class="c1"># same adjacency matrix (up to reorder of the nodes)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">adjplot</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">vtx_perm</span><span class="p">])]</span> <span class="p">[:,</span><span class="n">vtx_perm</span><span class="p">],</span> <span class="n">meta</span><span class="o">=</span><span class="n">meta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_28_0.png" src="../../_images/single-network-models_28_0.png" />
</div>
</div>
<p>Notice that now, the students are <em>not</em> organized according to school. We can see this by looking at the school assignment vector, shown at the left and top, of the network. It becomes pretty tough to figure out whether there are communities in our network just by looking at an adjacency matrix, unless you are looking at a network in which the nodes are <em>already arranged</em> in an order which respects the community structure.</p>
<p>In practice, this means that if you know ahead of time what natural groupings of the nodes might be (such knowing which school each student goes to) by way of your node attributes, you can visualize your data according to that grouping. If you don’t know anything about natural groupings of nodes, however, we are left with the problem of <em>estimating community structure</em>. A later method, called the <em>spectral embedding</em>, will be paired with clustering techniques to allow us to estimate node assignment vectors.</p>
<div class="section" id="id2">
<h4>*Likelihood<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>What does the likelihood for the <em>a priori</em> SBM look like? Fortunately, since <span class="math notranslate nohighlight">\(\vec \tau\)</span> is a <em>parameter</em> of the <em>a priori</em> SBM, the likelihood is a bit simpler than for the <em>a posteriori</em> SBM. This is because the <em>a posteriori</em> SBM requires a marginalization over potential realizations of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, whereas the <em>a priori</em> SBM does not. First, remember that the edges are independent. This means that the probability that <span class="math notranslate nohighlight">\(\mathbf A = A\)</span> can be split into the product of the probabilities that each entry <span class="math notranslate nohighlight">\(\mathbf a_{ij} = a_{ij}\)</span>. Next, remember that, given <span class="math notranslate nohighlight">\(\mathbf B\)</span> and <span class="math notranslate nohighlight">\(\vec \tau\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> takes the distribution <span class="math notranslate nohighlight">\(Bern(b_{\ell k})\)</span>, where <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>. Remember that the probability mass function of a Bernoulli R.V. is given by <span class="math notranslate nohighlight">\(\mathbb P(\mathbf x = x; p) = p^x (1 - p)^{1 - x}\)</span>. Putting these steps together gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal L_\theta(A) &amp;\propto \mathbb P_{\theta}(\mathbf A = A) \\
&amp;= \prod_{j &gt; i} \mathbb P_\theta(\mathbf a_{ij} = a_{ij}),\;\;\;\;\textrm{Independence Assumption} \\
&amp;= \prod_{j &gt; i} b_{\ell k}^{a_{ij}}(1 - b_{\ell k})^{1 - a_{ij}},\;\;\;\;\textrm{p.m.f. of Bernoulli distribution} \\
&amp;= \prod_{k, \ell}b_{\ell k}^{m_{\ell k}}(1 - b_{\ell k})^{n_{\ell k} - m_{\ell k}}
\end{align*}\]</div>
<p>Where <span class="math notranslate nohighlight">\(n_{\ell k}\)</span> denotes the total number of edges possible between nodes assigned to community <span class="math notranslate nohighlight">\(\ell\)</span> and nodes assigned to community <span class="math notranslate nohighlight">\(k\)</span>. That is, <span class="math notranslate nohighlight">\(n_{\ell k} = \sum_{j &gt; i} \mathbb 1_{\tau_i = \ell}\mathbb 1_{\tau_j = k}\)</span>. Further, we will use <span class="math notranslate nohighlight">\(m_{\ell k}\)</span> to denote the total number of edges observed in <span class="math notranslate nohighlight">\(A\)</span>. That is, <span class="math notranslate nohighlight">\(m_{\ell k} = \sum_{j &gt; i}\mathbb 1_{\tau_i = l}\mathbb 1_{\tau_j = k}a_{ij}\)</span>. Note that for a single <span class="math notranslate nohighlight">\((\ell,k)\)</span> community pair, that the likelihood is analogous to the likelihood of a realization of an ER random variable.</p>
<!--- We can formalize this a bit more explicitly. If we let $A^{\ell k}$ be defined as the subgraph *induced* by the edges incident nodes in community $\ell$ and those in community $k$, then we can say that $A^{\ell k}$ is a directed ER random network, --->
<p>Like the ER model, there are again equivalence classes of the sample space <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> in terms of their likelihood. For a two-community setting, with <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> given, the equivalence classes are the sets:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E_{a,b,c}(\vec \tau, B) &amp;= \left\{A \in \mathcal A_n : m_{11} = a, \m_{21}=m_{12} = b, m_{22} = c\right\}
\end{align*}\]</div>
<p>The number of equivalence classes possible scales with the number of communities, and the manner in which nodes are assigned to communities (particularly, the number of nodes in each community).</p>
</div>
</div>
<div class="section" id="a-posteriori-stochastic-block-model">
<h3><em>A Posteriori</em> Stochastic Block Model<a class="headerlink" href="#a-posteriori-stochastic-block-model" title="Permalink to this headline">¶</a></h3>
<p>In the <em>a posteriori</em> Stochastic Block Model (SBM), we consider that node assignment to one of <span class="math notranslate nohighlight">\(K\)</span> communities is a random process. This model has the following parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vec \pi\)</span></p></td>
<td><p>the <span class="math notranslate nohighlight">\(K-1\)</span> probability simplex</p></td>
<td><p>The probability of a node being assigned to community <span class="math notranslate nohighlight">\(K\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{K \times K}\)</span></p></td>
<td><p>The block matrix, which assigns edge probabilities for pairs of communities</p></td>
</tr>
</tbody>
</table>
<p>The <em>a posteriori</em> SBM is a bit more complicated than the <em>a priori</em> SBM. We will think about the <em>a posteriori</em> SBM as a variation of the <em>a priori</em> SBM, where instead of the node-assignment vector being treated as a fixed parameter, we will treat it as a vector-valued random variable. This vector-valued random variable, denoted by <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, is a <em>latent variable</em>, which means that in our real data, we don’t actually get to see realizations of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> itself. In this case, <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> takes values in the space <span class="math notranslate nohighlight">\(\{1,...,K\}^n\)</span>. This means that for a given realization of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, denoted by <span class="math notranslate nohighlight">\(\vec \tau\)</span>, that for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes in the network, we suppose that an integer value between <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(K\)</span> indicates which community a node is from. Statistically, we write that the node assignment for node <span class="math notranslate nohighlight">\(i\)</span>, denoted by <span class="math notranslate nohighlight">\(\pmb \tau_i\)</span>, is sampled independently and identically from <span class="math notranslate nohighlight">\(Categorical(\vec \pi)\)</span>. Stated another way, the vector <span class="math notranslate nohighlight">\(\vec\pi\)</span> indicates the probability <span class="math notranslate nohighlight">\(\pi_k\)</span> of assignment to each community <span class="math notranslate nohighlight">\(k\)</span> in the network.</p>
<p>The matrix <span class="math notranslate nohighlight">\(B\)</span> behaves exactly the same as it did with the <em>a posteriori</em> SBM. Finally, let’s think about how to write down the generative model in the <em>a posteriori</em> SBM. We showed above the model for the random node-assignment variable, <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, above. The model for edges of the <em>a posteriori</em> SBM is, in fact, conditional on the assignments obtained through this procedure. We write down that, conditioned on <span class="math notranslate nohighlight">\(\pmb \tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\pmb \tau_j = k\)</span>, that if <span class="math notranslate nohighlight">\(j &gt; i\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(b_{\ell,k})\)</span> distribution. What this reflects is the idea that, if node <span class="math notranslate nohighlight">\(i\)</span> is in community <span class="math notranslate nohighlight">\(\ell\)</span> and node <span class="math notranslate nohighlight">\(j\)</span> is in community <span class="math notranslate nohighlight">\(k\)</span>, that the <span class="math notranslate nohighlight">\((\ell,k)\)</span> entry of the block matrix <span class="math notranslate nohighlight">\(B\)</span> indicates the probability that nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are connected. Like the ER model, the edges are still independent, which means that edges occuring or not occuring do not have an impact on one another. Unlike above, we can no longer say that the <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>s are identically distributed, since the probability <span class="math notranslate nohighlight">\(b_{\ell, k}\)</span> depends on the communities that nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are assigned to. This means that if we were to look at a different pair of nodes, the probability that the two nodes are connected could be different, and therefore they might not share the same distribution. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an <em>a posteriori</em> SBM network with parameters <span class="math notranslate nohighlight">\(\vec \pi\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim SBM_n(\vec \pi, B)\)</span>.</p>
<div class="section" id="id3">
<h4>*Likelihood<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>What does the likelihood for the <em>a posteriori</em> SBM look like? In this case, <span class="math notranslate nohighlight">\(\theta = (\vec \pi, B)\)</span> are the parameters for the model, so the likelihood for a realization <span class="math notranslate nohighlight">\(A\)</span> of <span class="math notranslate nohighlight">\(\mathbf A\)</span> is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal L_\theta(A) &amp;\propto \mathbb P_\theta(\mathbf A = A)
\end{align*}\]</div>
<p>Next, we use the fact that the probability that <span class="math notranslate nohighlight">\(\mathbf A = A\)</span> is, in fact, the <em>marginalization</em> (over realizations of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>) of the joint <span class="math notranslate nohighlight">\((\mathbf A, \vec{\pmb \tau})\)</span>. In this case, we will let <span class="math notranslate nohighlight">\(\mathcal T = \{1,...,K\}^n\)</span> be the space of all possible realizations that <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> could take. In the line after that, we use Bayes’ Theorem to separate the joint probability into a conditional probability and a marginal probability:</p>
<div class="amsmath math notranslate nohighlight" id="equation-61d24b16-9f91-4d63-8c00-0b8e04ca78e4">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-61d24b16-9f91-4d63-8c00-0b8e04ca78e4" title="Permalink to this equation">¶</a></span>\[\begin{align}
    &amp;= \sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A, \vec{\pmb \tau} = \vec \tau) \\
    &amp;= \sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau)\mathbb P_\theta(\vec{\pmb \tau} = \vec \tau)
\end{align}\]</div>
<p>Let’s think about each of these probabilities separately. Remember that for <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>, that each entry <span class="math notranslate nohighlight">\(\pmb \tau_i\)</span> is sampled <em>independently and identically</em> from <span class="math notranslate nohighlight">\(Categorical(\vec \pi)\)</span>.The probability mass for a <span class="math notranslate nohighlight">\(Categorical(\vec \pi)\)</span>-valued random variable is <span class="math notranslate nohighlight">\(\mathbb P(\pmb \tau_i = \tau_i; \vec \pi) = \pi_{\tau_i}\)</span>. Finally, note that if we are taking the products of <span class="math notranslate nohighlight">\(n\)</span> <span class="math notranslate nohighlight">\(\pi_{\tau_i}\)</span> terms, that many of these values will end up being the same. Consider, for instance, if the vector <span class="math notranslate nohighlight">\(\tau = [1,2,1,2,1]\)</span>. We end up with three terms of <span class="math notranslate nohighlight">\(\pi_1\)</span>, and two terms of <span class="math notranslate nohighlight">\(\pi_2\)</span>, and it does not matter which order we multiply them in. Rather, all we need to keep track of are the counts of each <span class="math notranslate nohighlight">\(\pi\)</span>. term. Written another way, we can use the indicator that <span class="math notranslate nohighlight">\(\tau_i = k\)</span>, given by <span class="math notranslate nohighlight">\(\mathbb 1_{\tau_i = k}\)</span>, and a running counter over all of the community probability assignments <span class="math notranslate nohighlight">\(\pi_k\)</span> to make this expression a little more sensible. We will use the symbol <span class="math notranslate nohighlight">\(n_k = \sum_{i = 1}^n \mathbb 1_{\tau_i = k}\)</span> to denote this value:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(\vec{\pmb \tau} = \vec \tau) &amp;= \prod_{i = 1}^n \mathbb P_\theta(\pmb \tau_i = \tau_i),\;\;\;\;\textrm{Independence Assumption} \\
&amp;= \prod_{i = 1}^n \pi_{\tau_i} ,\;\;\;\;\textrm{p.m.f. of a Categorical R.V.}\\
&amp;= \prod_{k = 1}^K \pi_{k}^{n_k},\;\;\;\;\textrm{Reorganizing what we are taking products of}
\end{align*}\]</div>
<p>Next, let’s think about the conditional probability term, <span class="math notranslate nohighlight">\(\mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau)\)</span>. Remember that the entries are all independent conditional on <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span> taking the value <span class="math notranslate nohighlight">\(\vec\tau\)</span>. Just like with the <em>a priori</em> SBM, we can use the independence assumption to write this probability as the product of the entry-wise probabilities. Further, remember that conditional on <span class="math notranslate nohighlight">\(\pmb \tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\pmb \tau_j = k\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is <span class="math notranslate nohighlight">\(Bern(b_{\ell,k})\)</span>. The distribution of <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> does <em>not</em> depend on any of the other entries of <span class="math notranslate nohighlight">\(\vec{\pmb \tau}\)</span>. Finally, remembering the p.m.f. of a Bernoulli R.V. gives:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau) &amp;= \prod_{j &gt; i}\mathbb P_\theta(\mathbf a_{ij} = a_{ij} \big | \vec{\pmb \tau} = \vec \tau),\;\;\;\;\textrm{Independence Assumption} \\
&amp;= \prod_{j &gt; i}\mathbb P_\theta(\mathbf a_{ij} = a_{ij} \big | \pmb \tau_i = \ell, \pmb \tau_j = k) ,\;\;\;\;\textrm{$\mathbf a_{ij}$ depends only on $\tau_i$ and $\tau_j$}\\
&amp;= \prod_{j &gt; i} b_{\ell k}^{a_{ij}} (1 - b_{\ell k})^{1 - a_{ij}}
\end{align*}\]</div>
<p>Again, we can simplify this expression a bit. Recall the indicator function above. Let <span class="math notranslate nohighlight">\(m_{\ell k}(A0\)</span> and <span class="math notranslate nohighlight">\(n_{\ell k}\)</span> be defined as above, with the same interpretations. This expression can be simplified to:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau) &amp;= \prod_{\ell,k} b_{\ell k}^{m_{\ell k}}(1 - b_{\ell k})^{n_{\ell k} - m_{\ell k}}
\end{align*}\]</div>
<p>Combining these into the integrand gives:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal L_\theta(A) &amp;\propto \sum_{\vec \tau \in \mathcal T} \mathbb P_\theta(\mathbf A = A \big | \vec{\pmb \tau} = \vec \tau) \mathbb P_\theta(\vec{\pmb \tau} = \vec \tau) \\
&amp;= \sum_{\vec \tau \in \mathcal T} \prod_{k = 1}^K \pi_k^{n_k}\cdot \prod_{\ell,k} b_{\ell k}^{m_{\ell k}}(1 - b_{\ell k})^{n_{\ell k} - m_{\ell k}}
\end{align*}\]</div>
<p>Evaluating this sum explicitly proves to be relatively tedious and is a bit outside of the scope of this book, so we will omit it here.</p>
<!-- TODO: return to add equivalence classes --></div>
</div>
</div>
<div class="section" id="random-dot-product-graph-rdpg">
<h2>Random Dot Product Graph (RDPG)<a class="headerlink" href="#random-dot-product-graph-rdpg" title="Permalink to this headline">¶</a></h2>
<p>Let’s imagine that we have a network which follows the <em>a priori</em> Stochastic Block Model. To make this example a little bit more concrete, let’s borrow the code example from above. The nodes of our network represent each of the <span class="math notranslate nohighlight">\(300\)</span> students in our network. The node assignment vector represents which of the two schools eaach student attends, where the first <span class="math notranslate nohighlight">\(150\)</span> students attend school <span class="math notranslate nohighlight">\(1\)</span>, and the second <span class="math notranslate nohighlight">\(150\)</span> students attend school <span class="math notranslate nohighlight">\(2\)</span>. Remember that <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> look like:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Tau, Node Assignment Vector&quot;</span><span class="p">,</span>
        <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_33_0.png" src="../../_images/single-network-models_33_0.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_block</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_34_0.png" src="../../_images/single-network-models_34_0.png" />
</div>
</div>
<p>Are there any other ways to describe this scenario, other than using both <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>?</p>
<p>What if we were to look at the probabilities for <em>every</em> pair of edges? Remember, for a given <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, that a network which is SBM can be generated using the approach that, given that <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(b_{\ell k})\)</span>. That is, every entry is Bernoulli, with the probability indicated by appropriate entry of the block matrix corresponding to the pair of communities each node is in. However, there’s another way we could write down this generative model. Suppose we had a <span class="math notranslate nohighlight">\(n \times n\)</span> probability matrix, where for every <span class="math notranslate nohighlight">\(j &gt; i\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p_{ji} = p_{ij}, p_{ij} = \begin{cases}
        b_{11} &amp; \tau_i = 1, \tau_j = 1 \\
        b_{12} &amp; \tau_i = 1, \tau_j = 2 \\
        b_{22} &amp; \tau_i = 2, \tau_j = 1
    \end{cases}
\end{align*}\]</div>
<p>We will call the matrix <span class="math notranslate nohighlight">\(P\)</span> the <em>probability matrix</em> whose <span class="math notranslate nohighlight">\(i^{th}\)</span> row and <span class="math notranslate nohighlight">\(j^{th}\)</span> column is the entry <span class="math notranslate nohighlight">\(p_{ij}\)</span>, as defined above. Stated another way, <span class="math notranslate nohighlight">\(P = (p_{ij})\)</span>. What does <span class="math notranslate nohighlight">\(P\)</span> look like?</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_prob</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">150</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">150</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">P</span><span class="p">[</span><span class="mi">150</span><span class="p">:</span><span class="mi">300</span><span class="p">,</span> <span class="mi">150</span><span class="p">:</span><span class="mi">300</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">3</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">150</span><span class="p">,</span><span class="mi">150</span><span class="p">:</span><span class="mi">300</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
<span class="n">P</span><span class="p">[</span><span class="mi">150</span><span class="p">:</span><span class="mi">300</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">150</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_prob</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probablity Matrix&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">299</span><span class="p">],</span>
              <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;300&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_36_0.png" src="../../_images/single-network-models_36_0.png" />
</div>
</div>
<p>As we can see, <span class="math notranslate nohighlight">\(P\)</span> captures a similar modular structure to the actual adjacency matrix corresponding to the SBM network. Also, <span class="math notranslate nohighlight">\(P\)</span> captures the probability of connections between each pair of students. Indeed, it is the case that <span class="math notranslate nohighlight">\(P\)</span> contains the information of both <span class="math notranslate nohighlight">\(\vec\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. This means that we can write down a generative model by specifying <em>only</em> <span class="math notranslate nohighlight">\(P\)</span>, and we no longer need to specify <span class="math notranslate nohighlight">\(\vec\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> at all. To write down the generative model in this way, we say that for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(p_{ij})\)</span> independently, where <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>.</p>
<p>What is so special about this formulation of the SBM problem? As it turns out, for a <em>symmetric</em> probability matrix <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(P\)</span> can be decomposed using a matrix <span class="math notranslate nohighlight">\(X\)</span>, where <span class="math notranslate nohighlight">\(P = X X^\top\)</span>. We will call a single row of <span class="math notranslate nohighlight">\(X\)</span> the vector <span class="math notranslate nohighlight">\(\vec x_i\)</span>. Remember, using this expression, each entry <span class="math notranslate nohighlight">\(p_{ij}\)</span> is the product <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span>, for all <span class="math notranslate nohighlight">\(i, j\)</span>. Like <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(X\)</span> has <span class="math notranslate nohighlight">\(n\)</span> rows, each of which corresponds to a single node in our network. However, the special property of <span class="math notranslate nohighlight">\(X\)</span> is that it doesn’t <em>necessarily</em> have <span class="math notranslate nohighlight">\(n\)</span> columns: rather, <span class="math notranslate nohighlight">\(X\)</span> often will have many fewer columns than rows. For instance, with <span class="math notranslate nohighlight">\(P\)</span> defined as above, there in fact exists an <span class="math notranslate nohighlight">\(X\)</span> with just <span class="math notranslate nohighlight">\(2\)</span> columns that can be used to describe <span class="math notranslate nohighlight">\(P\)</span>. This matrix <span class="math notranslate nohighlight">\(X\)</span> will be called the <em>latent position matrix</em>, and each row <span class="math notranslate nohighlight">\(\vec x_i\)</span> will be called the <em>latent position of a node</em>. Like previously, there are two types of RDPGs: one in which <span class="math notranslate nohighlight">\(X\)</span> is treated as <em>known</em>, and another in which <span class="math notranslate nohighlight">\(X\)</span> is treated as <em>unknown</em>.</p>
<p>Now, your next thought might be that this requires a <em>lot</em> more space to represent an SBM network, and you’d be right: <span class="math notranslate nohighlight">\(\vec \tau\)</span> has <span class="math notranslate nohighlight">\(n\)</span> entries, and <span class="math notranslate nohighlight">\(B\)</span> has <span class="math notranslate nohighlight">\(K \times K\)</span> entries, where <span class="math notranslate nohighlight">\(K\)</span> is typically much smaller than <span class="math notranslate nohighlight">\(n\)</span>. On the other hand, in this formulation, <span class="math notranslate nohighlight">\(P\)</span> has <span class="math notranslate nohighlight">\(\binom{n}{2}\)</span> entries, which is much bigger than <span class="math notranslate nohighlight">\(n + K \times K\)</span> (since <span class="math notranslate nohighlight">\(K\)</span> is usually much smaller than <span class="math notranslate nohighlight">\(n\)</span>). The advantage is that under this formulation, <span class="math notranslate nohighlight">\(P\)</span> doesn’t need to have this rigorous modular structure characteristic of SBM networks, and can look a <em>lot</em> more interesting. As we will see in later chapters, this network representation will prove extremely flexible for allowing us to capture networks that are fairly complex. Further, we can also perform analysis on the matrix <span class="math notranslate nohighlight">\(X\)</span> itself, which will prove very useful for estimation of SBMs.</p>
<div class="section" id="a-priori-rdpg">
<h3><em>A Priori</em> RDPG<a class="headerlink" href="#a-priori-rdpg" title="Permalink to this headline">¶</a></h3>
<p>The <em>a priori</em> Random Dot Product Graph is an RDPG in which we know <em>a priori</em> the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. The <em>a priori</em> RDPG has the following parameter:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(X\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \mathbb R^{n \times d}\)</span></p></td>
<td><p>The matrix of latent positions for each node <span class="math notranslate nohighlight">\(n\)</span>.</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(X\)</span> is called the <strong>latent position matrix</strong> of the RDPG. We write that <span class="math notranslate nohighlight">\(X \in \mathbb R^{n \times d}\)</span>, which means that it is a matrix with real values, <span class="math notranslate nohighlight">\(n\)</span> rows, and <span class="math notranslate nohighlight">\(d\)</span> columns. We will use the notation <span class="math notranslate nohighlight">\(\vec x_i\)</span> to refer to the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of <span class="math notranslate nohighlight">\(X\)</span>. <span class="math notranslate nohighlight">\(\vec x_i\)</span> is referred to as the <strong>latent position</strong> of a node <span class="math notranslate nohighlight">\(i\)</span>. Visually, this looks something like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X = \begin{bmatrix}
     \vec x_{1}^\top \\
     \vdots \\
     \vec x_n^\top
    \end{bmatrix}
\end{align*}\]</div>
<p>Noting that <span class="math notranslate nohighlight">\(X\)</span> has <span class="math notranslate nohighlight">\(d\)</span> columns, this implies that <span class="math notranslate nohighlight">\(\vec x_i \in  \mathbb R^d\)</span>, or that each node’s latent position is a real-valued <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector.</p>
<p>What is the generative model for the <em>a priori</em> RDPG? As we discussed above, given <span class="math notranslate nohighlight">\(X\)</span>, for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(\vec x_i^\top \vec x_j)\)</span> independently. If <span class="math notranslate nohighlight">\(i &lt; j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (<em>symmetry</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> (<em>hollow</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> RDPG with parameter <span class="math notranslate nohighlight">\(X\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim RDPG_n(X)\)</span>.</p>
<div class="section" id="id4">
<h4>Code Examples<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>We will let <span class="math notranslate nohighlight">\(X\)</span> be a little more complex than in our preceding example. Our <span class="math notranslate nohighlight">\(X\)</span> will produce a <span class="math notranslate nohighlight">\(P\)</span> that still <em>somewhat</em> has a modular structure, but not quite as much as before. Let’s assume, for instance, that we have <span class="math notranslate nohighlight">\(300\)</span> people who live along a very long road that is <span class="math notranslate nohighlight">\(100\)</span> miles long, and each person is <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span> of a mile apart. The nodes of our network represent the people who live along our assumed street. If two people are closer to one another, it might make sense to think that they have a higher probability of being friends. If two people are neighbors, we think that they will have a very high probability of being connected (almost <span class="math notranslate nohighlight">\(1\)</span>) and when people are very far apart, we think that they will have a very low probability of being connected (almost <span class="math notranslate nohighlight">\(0\)</span>). What could we use for <span class="math notranslate nohighlight">\(X\)</span>?</p>
<p>One possible approach would be to let each <span class="math notranslate nohighlight">\(\vec x_i\)</span> be defined as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x_i = \begin{bmatrix}
        \frac{300 - i}{300} \\
        \frac{i}{300}
    \end{bmatrix}
\end{align*}\]</div>
<p>For instance, <span class="math notranslate nohighlight">\(\vec x_1 = \begin{bmatrix}1 \\ 0\end{bmatrix}\)</span>, and <span class="math notranslate nohighlight">\(\vec x_{300} = \begin{bmatrix} 0 \\ 1\end{bmatrix}\)</span>. Note that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{1,300} = \vec x_1^\top \vec x_j = 1 \cdot 0 + 0 \cdot 1 = 0
\end{align*}\]</div>
<p>What happens in between?</p>
<p>Let’s consider another person, person <span class="math notranslate nohighlight">\(100\)</span>. Note that person <span class="math notranslate nohighlight">\(100\)</span> lives closer to person <span class="math notranslate nohighlight">\(1\)</span> than to person <span class="math notranslate nohighlight">\(300\)</span>.  Here, <span class="math notranslate nohighlight">\(\vec x_{100} = \begin{bmatrix} \frac{2}{3}\\ \frac{1}{3}\end{bmatrix}\)</span>. This gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{1,100} &amp;= \vec x_1^\top \vec x_{100} = \frac{2}{3}\cdot 1 + 0 \cdot \frac{1}{3} = \frac{2}{3} \\
p_{100, 300} &amp;= \vec x_{100}^\top x_{300} = \frac{2}{3} \cdot 0 + \frac 1 3 \cdot 1 = \frac 1 3
\end{align*}\]</div>
<p>This time, person <span class="math notranslate nohighlight">\(200\)</span> lives closer to person <span class="math notranslate nohighlight">\(300\)</span> than person <span class="math notranslate nohighlight">\(100\)</span>. With <span class="math notranslate nohighlight">\(\vec x_{200} = \begin{bmatrix}\frac{1}{3} \\ \frac{2}{3} \end{bmatrix}\)</span>, we obtain that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{1,200} &amp;= \vec x_1^\top \vec x_{200} = \frac{1}{3}\cdot 1 + 0 \cdot \frac{2}{3} = \frac{1}{3} \\
p_{200, 300} &amp;= \vec x_{100}^\top x_{300} = \frac{1}{3} \cdot 0 + \frac 2 3 \cdot 1 = \frac 2 3 \\
p_{100,200} &amp;= \vec x_{100}^\top x_{200} = \frac{2}{3} \cdot \frac 1 3 + \frac 1 3 \cdot \frac 2 3 = \frac 4 9
\end{align*}\]</div>
<p>So, intuitively, it seems like our probability matrix <span class="math notranslate nohighlight">\(P\)</span> will capture the intuitive idea we described above. First, we’ll take a look at <span class="math notranslate nohighlight">\(X\)</span>, and then we’ll look at <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># the number of nodes in our network</span>

<span class="c1"># design the latent position matrix X according to </span>
<span class="c1"># the rules we laid out previously</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">n</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">n</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_lp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">ylab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">99</span><span class="p">,</span> <span class="mi">199</span><span class="p">,</span> <span class="mi">299</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">,</span> <span class="s2">&quot;200&quot;</span><span class="p">,</span> <span class="s2">&quot;300&quot;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_lp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix, X&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_40_0.png" src="../../_images/single-network-models_40_0.png" />
</div>
</div>
<p>The latent position matrix <span class="math notranslate nohighlight">\(X\)</span> that we plotted above is <span class="math notranslate nohighlight">\(n \times d\)</span> dimensions. There are a number of approaches, other than looking at a heatmap of <span class="math notranslate nohighlight">\(X\)</span>, with which we can visualize <span class="math notranslate nohighlight">\(X\)</span> to derive insights as to its structure. When <span class="math notranslate nohighlight">\(d=2\)</span>, another popular visualization is to look at the latent position vectors, <span class="math notranslate nohighlight">\(\vec x_i\)</span>, as individual points in <span class="math notranslate nohighlight">\(2\)</span>-dimensional space. This will give us a scatter plot of <span class="math notranslate nohighlight">\(n\)</span> points, each of which has two coordinates:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_latents</span><span class="p">(</span><span class="n">latent_positions</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ss</span> <span class="o">=</span> <span class="mi">6</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">latent_positions</span><span class="p">[</span><span class="n">ss</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">latent_positions</span><span class="p">[</span><span class="n">ss</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                           <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Set1&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plot</span>

<span class="c1"># plot</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix, X&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_42_0.png" src="../../_images/single-network-models_42_0.png" />
</div>
</div>
<p>The above scatter plot has been subsampled to show only every <span class="math notranslate nohighlight">\(6^{th}\)</span> latent position vector, so that the individual <span class="math notranslate nohighlight">\(2\)</span>-dimensional latent position vectors are discernable. Due to the way we constructed <span class="math notranslate nohighlight">\(X\)</span>, the scatter plot would otherwise appear to be a line (due to points overlapping one another). The reason that the points fall along a vertical line when plotted as a vector is due to the method we used to construct entries of <span class="math notranslate nohighlight">\(X\)</span>, described above. Next, we will look at the probability matrix:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_prob</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix, P=$XX^T$&quot;</span><span class="p">,</span>
         <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">,</span> <span class="s2">&quot;200&quot;</span><span class="p">,</span> <span class="s2">&quot;300&quot;</span><span class="p">],</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">99</span><span class="p">,</span><span class="mi">199</span><span class="p">,</span><span class="mi">299</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_44_0.png" src="../../_images/single-network-models_44_0.png" />
</div>
</div>
<p>Finally, we will sample an RDPG:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>

<span class="c1"># sample an RDPG with the latent position matrix</span>
<span class="c1"># created above</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">binary_heatmap</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$RDPG_</span><span class="si">{300}</span><span class="s2">(X)$ Simulation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_46_0.png" src="../../_images/single-network-models_46_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="id5">
<h3>*Likelihood<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X\)</span>, the likelihood for an RDPG is relatively straightforward. The independence assumption vastly simplifies our resulting expression. We will also use many of the results we’ve identified above, such as the p.m.f. of a Bernoulli random variable:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal L_\theta(A) &amp;\propto \mathbb P_\theta(A) \\
    &amp;= \prod_{j &gt; i}\mathbb P(\mathbf a_{ij} = a_{ij}),\;\;\;\; \textrm{Independence Assumption} \\
    &amp;= \prod_{j &gt; i}(\vec x_i^\top \vec x_j)^{a_{ij}}(1 - \vec x_i^\top \vec x_j)^{1 - a_{ij}},\;\;\;\; a_{ij} \sim Bern(\vec x_i^\top \vec x_j)
\end{align*}\]</div>
<p>Unfortunately, the likelihood equivalence classes are a bit harder to understand intuitionally here compared to the ER and SBM examples so we won’t write them down here, but they still exist!</p>
</div>
<div class="section" id="a-posteriori-rdpg">
<h3><em>A Posteriori</em> RDPG<a class="headerlink" href="#a-posteriori-rdpg" title="Permalink to this headline">¶</a></h3>
<p>The <em>a posteriori</em> RDPG is to the <em>a priori</em> RDPG what the <em>a posteriori</em> SBM was to the <em>a priori</em> SBM. Formally, we instead suppose that we do <em>not</em> know the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, but instead know how we can characterize the individual latent position vectors. We have the following parameter:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>F</p></td>
<td><p>inner-product distributions</p></td>
<td><p>A distribution for each latent position vector.</p></td>
</tr>
</tbody>
</table>
<p>The parameter <span class="math notranslate nohighlight">\(F\)</span> is what is known as an <strong>inner-product distribution</strong>. In the simplest case, we will assume that <span class="math notranslate nohighlight">\(F\)</span> is a distribution on a subset of the possible real vectors that have <span class="math notranslate nohighlight">\(d\)</span>-dimensions with an important caveat: for any two vectors within this subset, their inner product <em>must</em> be a probability. We will refer to the subset of the possible real vectors as <span class="math notranslate nohighlight">\(\mathcal X_d\)</span>, which is a subset of <span class="math notranslate nohighlight">\(\mathbb R^d\)</span>. This means that for any <span class="math notranslate nohighlight">\(\vec x_i, \vec x_j\)</span> that are in <span class="math notranslate nohighlight">\(\mathcal X_d\)</span>, it is always the case that <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span> is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. This is essential because like previously, we will describe the distribution of each edge in the adjacency matrix using <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span>. Next, we will treat the latent position matrix as a matrix-valued random variable which is <em>latent</em> (remember, <em>latent</em> means that we don’t get to see it in our real data). Like before, we will call <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i\)</span> the random latent position vectors for the nodes of our network. In this case, each <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span> is sampled independently and identically from the inner-product distribution <span class="math notranslate nohighlight">\(F\)</span> described above. The latent-position matrix is the matrix-valued random variable <span class="math notranslate nohighlight">\(\mathbf X\)</span> whose entries are the latent vectors <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span>, for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes.</p>
<p>The model for edges of the <em>a posteriori</em> RDPG can be described by conditioning n this unobserved latent-position matrix. We write down that, conditioned on <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i = \vec x\)</span> and <span class="math notranslate nohighlight">\(\vec {\mathbf x}_j = \vec y\)</span>, that if <span class="math notranslate nohighlight">\(j &gt; i\)</span>, then <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(\vec x^\top \vec y)\)</span> distribution. As before, if <span class="math notranslate nohighlight">\(i &lt; j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (<em>symmetry</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> (<em>hollow</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an <em>a posteriori</em> RDPG with parameter <span class="math notranslate nohighlight">\(F\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim RDPG_n(F)\)</span>.</p>
<div class="section" id="id6">
<h4>*Likelihood<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>The likelihood for the <em>a posteriori</em> RDPG is fairly complicated. This is because, like the <em>a posteriori</em> SBM, we do not actually get to see the latent position matrix <span class="math notranslate nohighlight">\(\mathbf X\)</span>, so we need to use <em>marginalization</em> to obtain an expression for the likelihood. Here, we are concerned with realizations of <span class="math notranslate nohighlight">\(\mathbf X\)</span>. Remember that <span class="math notranslate nohighlight">\(\mathbf X\)</span> is just a matrix whose rows are <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span>, each of which individually have have the distribution <span class="math notranslate nohighlight">\(F\)</span>; e.g., <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i \sim F\)</span>. For simplicity, we will assume that <span class="math notranslate nohighlight">\(F\)</span> is a disrete distribution on <span class="math notranslate nohighlight">\(\mathcal X_d\)</span>. This makes the logic of what is going on below much simpler since the notation gets less complicated, but does not detract from the generalizability of the result (the only difference is that sums would be replaced by multivariate integrals, and probability mass functions replaced by probability density functions).</p>
<p>We will let <span class="math notranslate nohighlight">\(p\)</span> denote the probability mass function (p.m.f.) of this discrete distribution function <span class="math notranslate nohighlight">\(F\)</span>. The strategy will be to use the independence assumption, followed by marginalization over the relevant rows of <span class="math notranslate nohighlight">\(\mathbf X\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal L_\theta(A) &amp;\propto \mathbb P_\theta(\mathbf A = A) \\
&amp;= \prod_{j &gt; i} \mathbb P(\mathbf a_{ij} = a_{ij}), \;\;\;\;\textrm{Independence Assumption} \\
\mathbb P(\mathbf a_{ij} = a_{ij})&amp;= \sum_{\vec x \in \mathcal X_d}\sum_{\vec y \in \mathcal X_d}\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y),\;\;\;\;\textrm{Marginalization over }\vec {\mathbf x}_i \textrm{ and }\vec {\mathbf x}_j
\end{align*}\]</div>
<p>Next, we will simplify this expression a little bit more, but using Bayes’ Theorem two times:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\\
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;= \mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) \mathbb P(\vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) \\
&amp;= \mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) \mathbb P(\vec{\mathbf x}_i = \vec x| \vec{\mathbf x}_j = \vec y) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Next, we we use a relatively clever application of Bayes’ theorem, and the definition of a conditional probability. Remember that for two discrete random variables <span class="math notranslate nohighlight">\(\mathbf a\)</span> and <span class="math notranslate nohighlight">\(\mathbf b\)</span>, that the conditional probability of <span class="math notranslate nohighlight">\(\mathbf a\)</span> taking the value <span class="math notranslate nohighlight">\(a\)</span> given that <span class="math notranslate nohighlight">\(\mathbf b = b\)</span> is defined as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P(\mathbf a = a| \mathbf b = b) &amp;= \frac{\mathbb P(\mathbf a = a, \mathbf b = b)}{\mathbb P(\mathbf b = b)}
\end{align*}\]</div>
<p>Further, remember that if <span class="math notranslate nohighlight">\(\mathbf a\)</span> and <span class="math notranslate nohighlight">\(\mathbf b\)</span> are independent, then <span class="math notranslate nohighlight">\(\mathbb P(\mathbf a = a, \mathbf b = b) = \mathbb P(\mathbf a = a)\mathbb P(\mathbf b = b)\)</span>. Using this fact:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P(\mathbf a = a| \mathbf b = b) &amp;= \frac{\mathbb P(\mathbf a = a)\mathbb P(\mathbf b = b)}{\mathbb P(\mathbf b = b)},\;\;\;\;\textrm{Independence} \\
    \Rightarrow \mathbb P(\mathbf a = a)\mathbb P(\mathbf b = b) &amp;= \mathbb P(\mathbf a = a| \mathbf b = b)\mathbb P(\mathbf b = b)
\end{align*}\]</div>
<p>We will use this result, combined with the fact that <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span> and <span class="math notranslate nohighlight">\(\vec {\mathbf x}_j\)</span> are independent, to greatly simplify the expression we had before. Note that with this fact, we obtain that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y) &amp;= \mathbb P(\vec{\mathbf x}_i = \vec x| \vec{\mathbf x}_j = \vec y) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Which means that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;=  \mathbb P(\mathbf a_{ij} = a_{ij} | \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y)\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Finally, using that <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is a Bernoulli R.V. conditional on <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span> and <span class="math notranslate nohighlight">\(\vec{\mathbf x}_j\)</span>, and that <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i\)</span> and <span class="math notranslate nohighlight">\(\vec{\mathbf x}_j\)</span> have p.m.f. <span class="math notranslate nohighlight">\(p\)</span>, we obtain:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;=  (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y),\;\;\;\;\mathbf a_{ij}|\vec x, \vec{y}\textrm{ has $Bern(\vec x^\top \vec y)$ distribution} \\
&amp;=  (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}p(\vec x)p(\vec y),\;\;\;\;\vec{\mathbf x}_i, \vec{\mathbf x}_j\textrm{ have p.m.f. $p$}
\end{align*}\]</div>
<p>So our complete expression for the likelihood is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal L_\theta(A) &amp;\propto \prod_{j &gt; i}\sum_{\vec x \in \mathcal X_d}\sum_{\vec y \in \mathcal X_d} (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}p(\vec x)p(\vec y)
\end{align*}\]</div>
</div>
</div>
</div>
<div class="section" id="inhomogeneous-erdos-renyi-ier">
<h2>Inhomogeneous Erdös-Rényi (IER)<a class="headerlink" href="#inhomogeneous-erdos-renyi-ier" title="Permalink to this headline">¶</a></h2>
<p>In the preceding models, we typically made assumptions about how we could characterize the edge-existence probabilities using fewer than <span class="math notranslate nohighlight">\(\binom n 2\)</span> unique probabilities (one for each edge). The reason for this is that in general, <span class="math notranslate nohighlight">\(n\)</span> is usually relatively large, so attempting to actually learn <span class="math notranslate nohighlight">\(\binom n 2\)</span> unique probabilities is not, in general, going to be very feasible (it is <em>never</em> feasible when we have a single network, since a single network only has <span class="math notranslate nohighlight">\(\binom n 2\)</span> unique observed values). Further, it is relatively difficult to ask questions for which assuming edges share <em>nothing</em> in common (even if they don’t share the same probabilities, there may be properties underlying the probabilities, such as the <em>latent positions</em> that we saw above with the RDPG, that we might still want to characterize) is actually favorable.</p>
<p>Nonetheless, the most general model for an independent-edge random network is known as the Inhomogeneous Erdös-Rényi (IER) Random Network. An IER Random Network is characterized by the following parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(P\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{n \times n}\)</span></p></td>
<td><p>The edge probability matrix.</p></td>
</tr>
</tbody>
</table>
<p>The probability matrix <span class="math notranslate nohighlight">\(P\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix, where each entry <span class="math notranslate nohighlight">\(p_{ij}\)</span> is a probability (a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>). Further, if we restrict ourselves to the case of simple networks like we have done so far, <span class="math notranslate nohighlight">\(P\)</span> will also be symmetric (<span class="math notranslate nohighlight">\(p_{ij} = p_{ji}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>). The generative model is similar to the preceding models we have seen: given the <span class="math notranslate nohighlight">\((i, j)\)</span> entry of <span class="math notranslate nohighlight">\(P\)</span>, denoted <span class="math notranslate nohighlight">\(p_{ij}\)</span>, the edges <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> are independent <span class="math notranslate nohighlight">\(Bern(p_{ij})\)</span>, for any <span class="math notranslate nohighlight">\(j &gt; i\)</span>. Further, <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> (hollow adjacency matrix, and loopless network), and <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (symmetric adjacency matrix, and undirected network). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency maatrix for an IER network with probability matarix <span class="math notranslate nohighlight">\(P\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim IER_n(P)\)</span>.</p>
<p>It is worth noting that <em>all</em> of the preceding models we have discussed so far are special cases of the IER model. This means that, for instance, if we were to consider only the probability matrices where all of the entries are the same, we could represent the ER models. Similarly, if we were to only to consider the probability matrices <span class="math notranslate nohighlight">\(P\)</span> where <span class="math notranslate nohighlight">\(P = XX^\top\)</span>, we could represent any RDPG.</p>
<div class="section" id="id7">
<h3>*Likelihood<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>The likelihood for a network which is IER is very straightforward. We use the independence assumption, and the p.m.f. of a Bernoulli-distributed random-variable <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal L_\theta(A) &amp;\propto \mathbb P(\mathbf A = A) \\
    &amp;= \prod_{j &gt; i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - a_{ij}}
\end{align*}\]</div>
</div>
</div>
<div class="section" id="degree-corrected-stochastic-block-model-dcsbm">
<h2>Degree-Corrected Stochastic Block Model (DCSBM)<a class="headerlink" href="#degree-corrected-stochastic-block-model-dcsbm" title="Permalink to this headline">¶</a></h2>
<p>Let’s think back to our school example for the Stochastic Block Model. Remember, we had 100 students, each of whom could go to one of two possible schools: school one or school two. Our network had 100 nodes, representing each of the students. We said that the school for which each student attended was represented by their node assignment <span class="math notranslate nohighlight">\(\tau_i\)</span> to one of two possible communities. The matrix <span class="math notranslate nohighlight">\(B\)</span> was the block probaability matrix, where <span class="math notranslate nohighlight">\(b_{11}\)</span> was the probability that students in school one were friends, <span class="math notranslate nohighlight">\(b_{22}\)</span> was the probability that students in school two were friends, and <span class="math notranslate nohighlight">\(b_{12} = b_{21}\)</span> was the probability that students were friends if they did not go to the same school. In this case, we said that <span class="math notranslate nohighlight">\(\mathbf A \sim SBM_n(\tau, B)\)</span>.</p>
<p>What is a scenario in which this model might not make sense? Let’s say that Alice and Bob both go to the same school, but Alice is more popular than Bob. If we were to look at a schoolmate Chadwick, it might not make sense to say that both Alice and Bob have the <em>same</em> probability of being friends with Chadwick. Rather, we might want to reflect that Alice has a higher probability of being friends with an arbitrary schoolmate than Bob. The problem here is that within a single community of an SBM, the SBM assumes that the <strong>node degree</strong> (which we examined for the ER network) is the <em>same</em> for all nodes within a single community.</p>
<div class="admonition-degree-homogeneity-in-a-stochastic-block-model-network admonition">
<p class="admonition-title">Degree Homogeneity in a Stochastic Block Model Network</p>
<p>Suppose that <span class="math notranslate nohighlight">\(\mathbf A \sim SBM_n(\tau, B)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf A\)</span> has <span class="math notranslate nohighlight">\(K=2\)</span> communities. What is the node degree of each node in <span class="math notranslate nohighlight">\(\mathbf A\)</span>?</p>
<p>For an arbitrary node <span class="math notranslate nohighlight">\(v_i\)</span> which is in community <span class="math notranslate nohighlight">\(k\)</span> (either <span class="math notranslate nohighlight">\(1\)</span> or <span class="math notranslate nohighlight">\(2\)</span>), we will compute the expectated value of the degree <span class="math notranslate nohighlight">\(deg(v_i)\)</span>, written <span class="math notranslate nohighlight">\(\mathbb E\left[deg(v_i); \tau_i = k\right]\)</span>. We will let <span class="math notranslate nohighlight">\(n_k\)</span> represent the number of nodes whose node assignments <span class="math notranslate nohighlight">\(\tau_i\)</span> are to community <span class="math notranslate nohighlight">\(k\)</span>. Let’s see what happens:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &amp;= \mathbb E\left[\sum_{j = 1}^n \mathbf a_{ij}\right] \\
    &amp;= \sum_{j = 1}^n \mathbb E[\mathbf a_{ij}]
\end{align*}\]</div>
<p>We use the <em>linearity of expectation</em> again to get from the top line to the second line. Next, instead of summing over all the vertices, we’ll break the sum up into the vertices which are in the same community as node <span class="math notranslate nohighlight">\(i\)</span>, and the ones in the <em>other</em> community <span class="math notranslate nohighlight">\(k'\)</span>. We use the notation <span class="math notranslate nohighlight">\(k'\)</span> to emphasize that <span class="math notranslate nohighlight">\(k\)</span> and <span class="math notranslate nohighlight">\(k'\)</span> are different values:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &amp;= \sum_{j : i \neq j, \tau_j = k} \mathbb E\left[\mathbf a_{ij}\right] + \sum_{j : \tau_j = \ell} \mathbb E[\mathbf a_{ij}]
\end{align*}\]</div>
<p>In the first sum, we have <span class="math notranslate nohighlight">\(n_k-1\)</span> total edges (the number of nodes that aren’t node <span class="math notranslate nohighlight">\(i\)</span>, but are in the same community), and in the second sum, we have <span class="math notranslate nohighlight">\(n_{k'}\)</span> total edges (the number of nodes that are in the other community). Finally, we will use that the probability of an edge in the same community is <span class="math notranslate nohighlight">\(b_{kk}\)</span>, but the probability of an edge between the communities is <span class="math notranslate nohighlight">\(b_{k' k}\)</span>. Finally, we will use that the expected value of an adjacency <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> which is Bernoulli distributed is its probability:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i); \tau_i = k\right] &amp;= \sum_{j : i \neq j, \tau_j = k} b_{kk} + \sum_{j : \tau_j = \ell} b_{kk'},\;\;\;\;\mathbf a_{ij}\textrm{ are Bernoulli distributed} \\
    &amp;= (n_k - 1)b_{kk} + n_{k'} b_{kk'}
\end{align*}\]</div>
<p>This holds for any node <span class="math notranslate nohighlight">\(i\)</span> which is in community <span class="math notranslate nohighlight">\(k\)</span>. Therefore, the node degree is the same, or <strong>homogeneous</strong>, within a community of an SBM.</p>
</div>
<p>To address this limitation, we turn to the Degree-Corrected Stochastic Block Model, or DCSBM. As with the Stochastic Block Model, there is both a <em>a priori</em> and <em>a posteriori</em> DCSBM.</p>
<div class="section" id="a-priori-dcsbm">
<h3><em>A Priori</em> DCSBM<a class="headerlink" href="#a-priori-dcsbm" title="Permalink to this headline">¶</a></h3>
<p>Like the <em>a priori</em> SBM, the <em>a priori</em> DCSBM is where we know which nodes are in which node communities ahead of time. Here, we will use the variable <span class="math notranslate nohighlight">\(K\)</span> to denote the maximum number of communities that nodes could be assigned to. The <em>a priori</em> DCSBM and has the following two parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vec\tau\)</span></p></td>
<td><p>{1,…,K}<span class="math notranslate nohighlight">\(^n\)</span></p></td>
<td><p>The community assignment vector for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes to one of <span class="math notranslate nohighlight">\(K\)</span> communities</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{K \times K}\)</span></p></td>
<td><p>The block matrix, which assigns edge probabilities for pairs of communities</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vec\theta\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathbb R^n_+\)</span></p></td>
<td><p>The degree correction vector, which adjusts the degree for pairs of nodes</p></td>
</tr>
</tbody>
</table>
<p>We write that <span class="math notranslate nohighlight">\(\vec \tau \in \{1, ..., K\}^n\)</span>, which means that <span class="math notranslate nohighlight">\(\vec \tau\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector which takes one of <span class="math notranslate nohighlight">\(K\)</span> possible values. In our social network example, for instance, this vector would reflect the fact that each of the <span class="math notranslate nohighlight">\(n\)</span> students in our network could be attendees at one of <span class="math notranslate nohighlight">\(K\)</span> possible schools. For a single node <span class="math notranslate nohighlight">\(i\)</span> that is in community <span class="math notranslate nohighlight">\(\ell\)</span>, where <span class="math notranslate nohighlight">\(\ell \in \{1, ..., K\}\)</span>, we write that <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span>.</p>
<p>Like for the SBM, <span class="math notranslate nohighlight">\(B\)</span> is the <strong>block matrix</strong> of the DCSBM. We write down that <span class="math notranslate nohighlight">\(B \in [0, 1]^{K \times K}\)</span>, which means that the block matrix is a matrix with <span class="math notranslate nohighlight">\(K\)</span> rows and <span class="math notranslate nohighlight">\(K\)</span> columns. If we have a pair of nodes and know which of the <span class="math notranslate nohighlight">\(K\)</span> communities each node is from, the block matrix tells us the probability that those two nodes are connected. If our networks are simple, the matrix <span class="math notranslate nohighlight">\(B\)</span> is also symmetric, which means that if <span class="math notranslate nohighlight">\(b_{k, \ell} = p\)</span> where <span class="math notranslate nohighlight">\(p\)</span> is a probability, that <span class="math notranslate nohighlight">\(b_{\ell, k} = p\)</span>, too. The requirement of <span class="math notranslate nohighlight">\(B\)</span> to be symmetric exists <em>only</em> if we are dealing with simple networks, since they are undirected; if we relax the requirement of undirectedness (and allow directed networks) <span class="math notranslate nohighlight">\(B\)</span> no longer need be symmetric.</p>
<p>The vector <span class="math notranslate nohighlight">\(\vec\theta\)</span> is the degree correction vector. Each entry <span class="math notranslate nohighlight">\(\theta_i\)</span> is a positive scalar. For every adjacency for a given node <span class="math notranslate nohighlight">\(i\)</span>, the degree correction <span class="math notranslate nohighlight">\(\theta_i\)</span> will indicate the factor by which the probability for an adjacency which represents an edge incident node <span class="math notranslate nohighlight">\(i\)</span> is adjusted.</p>
<p>Finally, let’s think about how to write down the generative model for the <em>a priori</em> DCSBM. We say that <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(\theta_i \theta_j b_{\ell, k})\)</span> distribution for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>. As we can see, <span class="math notranslate nohighlight">\(\theta_i\)</span> in a sense is “correcting” the probabilities of each adjacency to node <span class="math notranslate nohighlight">\(i\)</span> to be higher, or lower, depending on the value of <span class="math notranslate nohighlight">\(\theta_i\)</span> that that which is given by the block probabilities <span class="math notranslate nohighlight">\(b_{\ell k}\)</span>. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> SBM network with parameters <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim DCSBM_n(\vec \tau, B)\)</span>.</p>
</div>
</div>
<div class="section" id="network-models-for-networks-which-aren-t-simple">
<h2>Network Models for Networks which aren’t Simple<a class="headerlink" href="#network-models-for-networks-which-aren-t-simple" title="Permalink to this headline">¶</a></h2>
<p>To make the discussions a little more easy to handle, in the above descriptions, we described network models for simple networks, which to recap, are binary networks which are both loopless and undirected. Stated another way, simple networks are networks whose adjacency matrices are only <span class="math notranslate nohighlight">\(0\)</span>s and <span class="math notranslate nohighlight">\(1\)</span>s, they are hollow, and symmetric. What happens our networks don’t quite look this way?</p>
<p>For now, we’ll keep the assumption that the networks are binary, but we will discuss non-binary network models in a later chapter. We have three possibilities we can consider, and we will show how the “relaxations” of the assumptions change a description of a network model. We split these out so we can be as clear as possible about how the generative model changes.</p>
<p>We will compare each relaxation to the statement about the generative model for the ER generative model. To recap, for a simple network, we wrote:</p>
<p>Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <span class="math notranslate nohighlight">\(Bern(p)\)</span> distribution, whenever <span class="math notranslate nohighlight">\(j &gt; i\)</span>. When <span class="math notranslate nohighlight">\(i &gt; j\)</span>, we allow <span class="math notranslate nohighlight">\(\mathbf a_{ij} = \mathbf a_{ji}\)</span>. Also, we let <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>, which means that all self-loops are always unconnected.</p>
<div class="section" id="binary-network-model-which-has-loops-but-is-undirected">
<h3>Binary Network Model which has Loops, but is Undirected<a class="headerlink" href="#binary-network-model-which-has-loops-but-is-undirected" title="Permalink to this headline">¶</a></h3>
<p>Here, all we want to do is relax the assumption that the network is loopless. We simply ignore the statement that <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>, and allow that the <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> which follow a Bernoulli distribution (with some probability which depends on the network model choice) <em>now</em> applies to <span class="math notranslate nohighlight">\(j \geq i\)</span>, and not just <span class="math notranslate nohighlight">\(j &gt; i\)</span>. We keep that <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span>, which maintains the symmetry of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (and consequently, the undirectedness of the network).</p>
<p>Our description of the ER network changes to:</p>
<p>Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <span class="math notranslate nohighlight">\(Bern(p)\)</span> distribution, whenever <span class="math notranslate nohighlight">\(j \geq i\)</span>. When <span class="math notranslate nohighlight">\(i &gt; j\)</span>, we allow <span class="math notranslate nohighlight">\(\mathbf a_{ij} = \mathbf a_{ji}\)</span>.</p>
</div>
<div class="section" id="binary-network-model-which-is-loopless-but-directed">
<h3>Binary Network Model which is Loopless, but Directed<a class="headerlink" href="#binary-network-model-which-is-loopless-but-directed" title="Permalink to this headline">¶</a></h3>
<p>Like above, we simply ignore the statement that <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span>, which removes the symmetry of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (and consequently, removes the undirectedness of the network). We allow that the <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> which follows a Bernoulli distribution now apply to <span class="math notranslate nohighlight">\(j \neq i\)</span>, and not just <span class="math notranslate nohighlight">\(j &gt; i\)</span>. We keep that <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>, which maintains the hollowness of <span class="math notranslate nohighlight">\(\mathbf A\)</span> (and consequently, the undirectedness of the network).</p>
<p>Our description of the ER network changes to:</p>
<p>Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <span class="math notranslate nohighlight">\(Bern(p)\)</span> distribution, whenever <span class="math notranslate nohighlight">\(j \neq i\)</span>. Also, we let <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>, which means that all self-loops are always unconnected.</p>
</div>
<div class="section" id="binary-network-model-which-is-has-loops-and-is-directed">
<h3>Binary Network Model which is has Loops and is Directed<a class="headerlink" href="#binary-network-model-which-is-has-loops-and-is-directed" title="Permalink to this headline">¶</a></h3>
<p>Finally, for a network which has loops and is directed, we combine the above two approaches. We ignore the statements that <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span>, and the statement thhat <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>.</p>
<p>Our descriptiomn of the ER network changes to:</p>
<p>Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <span class="math notranslate nohighlight">\(Bern(p)\)</span> distribution, for all possible combinations of nodes <span class="math notranslate nohighlight">\(j\)</span> and <span class="math notranslate nohighlight">\(i\)</span>.</p>
</div>
</div>
<div class="section" id="generalized-random-dot-product-graph-grdpg">
<h2>Generalized Random Dot Product Graph (GRDPG)<a class="headerlink" href="#generalized-random-dot-product-graph-grdpg" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<p>[1] Erdös P, Rényi A. 1959. “On random graphs, I.” Publ. Math. Debrecen 6:290–297.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./representations/ch5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="why-use-models.html" title="previous page">Why Use Statistical Models?</a>
    <a class='right-next' id="next-link" href="multi-network-models.html" title="next page">Multi-Network Models</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Joshua Vogelstein, Alex Loftus, and Eric Bridgeford<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  None
  </body>
</html>