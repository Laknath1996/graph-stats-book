
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Single-Network Models &#8212; Network Machine Learning in Python</title>
    
  <link rel="stylesheet" href="../../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e7340bb3dbd8dde6db86f25597f54a1b.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.7d483ff0a819d6edff12ce0b1ead3928.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Multi-Network Models" href="multi-network-models.html" />
    <link rel="prev" title="Why Use Statistical Models?" href="why-use-models.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Network Machine Learning in Python</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../../intro.html">
   Network Machine Learning in Python
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Data Science Landscape
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-networks.html">
     1.4. Types of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.5. Types of Network Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/exercises.html">
     1.6. Exercises
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Data Science Project
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/discover-and-visualize.html">
     2.1. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.2. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/transformation-techniques.html">
     2.3. Transformation Techniques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.4. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.5. Fine-Tune your Model
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Data Science Project
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../ch4/ch4.html">
   1. Properties of Networks as a Statistical Object
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/matrix-representations.html">
     1.1. Matrix Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/network-representations.html">
     1.2. Network Representations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/properties-of-networks.html">
     1.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/regularization.html">
     1.4. Regularization
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="ch5.html">
   2. Why Use Statistical Models?
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="why-use-models.html">
     Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Single-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="#references">
     References
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi-network-models.html">
     Multi-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="models-with-covariates.html">
     Network Models with Covariates
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../ch6/ch6.html">
   3. Learning Graph Representations
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters.html">
     3.1. Estimating Parameters in Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/why-embed-networks.html">
     3.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/random-walk-diffusion-methods.html">
     3.3. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/graph-neural-networks.html">
     3.4. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/multigraph-representation-learning.html">
     3.5. Multigraph Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/joint-representation-learning.html">
     3.6. Joint Representation Learning
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../ch7/ch7.html">
   4. Theoretical Results
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-single-network.html">
     4.1. Theory for Single Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-multigraph.html">
     4.2. Theory for Multiple-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-matching.html">
     4.3. Theory for Graph Matching
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   1. Leveraging Representations for Single Graph Applications
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/community-detection.html">
     1.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/testing-differences.html">
     1.2. Testing for Differences between Communities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/model-selection.html">
     1.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/vertex-nomination.html">
     1.4. Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/anomaly-detection.html">
     1.5. Anomaly Detection
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   2. Leveraging Representations for Multiple Graph Applications
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/two-sample-hypothesis.html">
     2.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/graph-matching-vertex.html">
     2.2. Graph Matching and Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/vertex-nomination.html">
     2.3. Vertex Nomination
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1 collapsible-parent">
  <a class="reference internal" href="../../applications/ch10/ch10.html">
   3. Algorithms for more than 2 graphs
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-vertices.html">
     3.1. Testing for Significant Vertices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-communities.html">
     3.2. Testing for Significant Communities
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
   <i class="fas fa-external-link-alt">
   </i>
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/representations/ch5/single-network-models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/representations/ch5/single-network-models.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


            <!-- Full screen (wrap in <a> to have style consistency -->
            <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                    data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                    title="Fullscreen mode"><i
                        class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/representations/ch5/single-network-models.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i>
            Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Single-Network Models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#erdos-renyi-er">
     Erdös-Rényi (ER)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-block-model-sbm">
     Stochastic Block Model (SBM)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-dot-product-graph-rdpg">
     Random Dot Product Graph (RDPG)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalized-random-dot-product-graph-grdpg">
     Generalized Random Dot Product Graph (GRDPG)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#degree-corrected-models">
     Degree-Corrected Models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structured-independent-edge-model-siem">
     Structured Independent Edge Model (SIEM)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inhomogeneous-erdos-renyi-ier">
     Inhomogeneous Erdos-Renyi (IER)
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="single-network-models">
<h1>Single-Network Models<a class="headerlink" href="#single-network-models" title="Permalink to this headline">¶</a></h1>
<p>To understand single network models, it is crucial to understand the concept of the distribution of a network topology. We have a realization <span class="math notranslate nohighlight">\(A\)</span>, and we think that this realization is random in some way. Stated another way, we think that there exists soome random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> that governs the realizations we get to see. Since <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random variable, we can describe it using a probability distribution. The distribution of the network topology is the function <span class="math notranslate nohighlight">\(P\)</span> which, if <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an unweighted random network with <span class="math notranslate nohighlight">\(n\)</span> edges, assigns probabilities to every possible configuration that <span class="math notranslate nohighlight">\(\mathbf A\)</span> could possibly take. Notationally, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P\)</span>, which is read in words as “the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is distributed according to <span class="math notranslate nohighlight">\(\mathbb P\)</span>.”</p>
<p>In the preceding description, we made a fairly substantial claim: <span class="math notranslate nohighlight">\(P\)</span> assigns probabilities to every possible configuration that realizations of <span class="math notranslate nohighlight">\(\mathbf A\)</span>, denoted by <span class="math notranslate nohighlight">\(A\)</span>, could take. How many possibilities are there for a network with <span class="math notranslate nohighlight">\(n\)</span> edges? Is this set the same for any unweighted random network, or is it ever different? It turns out that the answer here is fairly straightforward: for any unweighted random network with <span class="math notranslate nohighlight">\(n\)</span> edges, the set of possible realizations, which we represent with the symbol <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>, is exactly the same! Formally, we describe <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> as:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal A_n \triangleq \left\{A : A \in \{0, 1\}^{n \times n}\right\} \equiv\{0, 1\}^{n \times n}
\end{align*}\]</div>
<p>In words, <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> is the set of all possible adjacency matrices <span class="math notranslate nohighlight">\(A\)</span> that correspond to unweighted networks with <span class="math notranslate nohighlight">\(n\)</span> nodes. Note that the two expressions written above are exactly the same, with the right-most expression being short-hand for the middle expression. To summarize the statement that <span class="math notranslate nohighlight">\(\mathbb P\)</span> assiigns probabilities to every possible configuration that realizations of <span class="math notranslate nohighlight">\(\mathbf A\)</span> can take, we write that <span class="math notranslate nohighlight">\(\mathbb P : \mathcal A_n \rightarrow [0, 1]\)</span>. This means that for any <span class="math notranslate nohighlight">\(A \in \mathcal A_n\)</span> which is a possible realization of <span class="math notranslate nohighlight">\(\mathbf A\)</span>, that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf A = A)\)</span> is a probability (it takes a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>). If it is completely unambiguous what the random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> refers to, we might use the short-hand <span class="math notranslate nohighlight">\(\mathbb P(\mathbf A = A) \equiv \mathbb P(A)\)</span>. This statement can alternatively be read that the probability that the random variable <span class="math notranslate nohighlight">\(\mathbf A\)</span> takes the value <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(\mathbb P(A)\)</span>. When you see the short-hand expression <span class="math notranslate nohighlight">\(\mathbb P(A)\)</span>, you should typically think back to the most recent random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> that has been discussed, and it is typically assumed that <span class="math notranslate nohighlight">\(\mathbb P\)</span> refers to the probability distribution of that random network; e.g., <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P\)</span>. Finally, let’s address that question we had in the previous paragraph. How many possible adjacency matrices are in <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>? Note that each adjacency <span class="math notranslate nohighlight">\(a_{ij}\)</span> can take one of two possible values. As there are <span class="math notranslate nohighlight">\(n\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns of <span class="math notranslate nohighlight">\(A\)</span> for any <span class="math notranslate nohighlight">\(A \in \mathcal A_n\)</span>, this comes to:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \left|\mathcal A_n\right| &amp;= \prod_{i = 1}^n \prod_{j = 1}^n 2 \\
    &amp;= \prod_{i = 1}^n 2^n \\
    &amp;= 2^{n \times n} = 2^{n^2}
\end{align*}\]</div>
<p>When <span class="math notranslate nohighlight">\(n\)</span> is just <span class="math notranslate nohighlight">\(10\)</span>, note that <span class="math notranslate nohighlight">\(\left|\mathcal A_{10}\right| = 2^{10^2} = 2^{100}\)</span>, which is more than <span class="math notranslate nohighlight">\(10^{30}\)</span> possible networks that can be realized!</p>
<p>So, now we know that we have probability distributions on networks, and a set <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> which defines all of the aadjacency matrices that every probability distribution must assign a probability to. Now, just what is a single network model? The <strong>single network model</strong> is the tuple <span class="math notranslate nohighlight">\((\mathcal A_n, \mathcal P)\)</span>. Above, we learned that <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> was the set of all possible adjacency matrices for unweighted networks with <span class="math notranslate nohighlight">\(n\)</span> nodes. We will call <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> the <strong>sample space</strong> of <span class="math notranslate nohighlight">\(n\)</span>-node networks. In general, <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> will be the same sample space for all <span class="math notranslate nohighlight">\(n\)</span>-node network models. This means that for any <span class="math notranslate nohighlight">\(n\)</span>-node network realization <span class="math notranslate nohighlight">\(A\)</span>, we can calculate a probability that <span class="math notranslate nohighlight">\(A\)</span> is described by any probability distribution on <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> found in <span class="math notranslate nohighlight">\(\mathcal P\)</span>. What is <span class="math notranslate nohighlight">\(\mathcal P\)</span>? It depends on the model we want to use! In general, <span class="math notranslate nohighlight">\(\mathcal P\)</span> has only one rule: it is a nonempty set (it contains at least <em>something</em>), where for every <span class="math notranslate nohighlight">\(\mathbb P \in \mathcal P\)</span>, <span class="math notranslate nohighlight">\(\mathbb P\)</span> is a probability distribution on <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>. Not that this says only that <span class="math notranslate nohighlight">\(\mathcal P\)</span> cannot be empty, but it doesn’t say anything about how big or diverse it can be! In general, we will simplify <span class="math notranslate nohighlight">\(\mathcal P\)</span> through something called <em>parametrization</em>; that is, we will write <span class="math notranslate nohighlight">\(\mathcal P\)</span> as the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal P(\Theta) &amp;\triangleq \left\{\mathbb P_\theta : \theta \in \Theta\right\}
\end{align*}\]</div>
<p>Where <span class="math notranslate nohighlight">\(\Theta\)</span> are the set of all possible parameters of the random network model, and <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> is a particular parameter choice that governs the parameters of a specific random network <span class="math notranslate nohighlight">\(\mathbf A\)</span>. That is, if <span class="math notranslate nohighlight">\(\mathbf A\)</span> is a random network that follows a single network model, we will write that <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P_\theta\)</span>, for some choice <span class="math notranslate nohighlight">\(\theta\)</span>. As above, if it is totally unambiguous what <span class="math notranslate nohighlight">\(\theta\)</span> refers to, we will just use the shorthand <span class="math notranslate nohighlight">\(\mathbf A \sim \mathbb P\)</span>. Another form of shorthand that comes up sometimes is, if we are referring to the probability of a particular realization <span class="math notranslate nohighlight">\(A\)</span> of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> with parameters <span class="math notranslate nohighlight">\(\theta\)</span>, we will write <span class="math notranslate nohighlight">\(\mathbb P_\theta(\mathbf A = A)\)</span> instead as <span class="math notranslate nohighlight">\(\mathbb P(\mathbf A = A; \theta)\)</span> or <span class="math notranslate nohighlight">\(\mathbb P(A; \theta)\)</span> when the random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> is obvious. We use a semi-colon to denote that the parameters <span class="math notranslate nohighlight">\(\theta\)</span> are supposed to be <strong>fixed</strong> quantities for a given <span class="math notranslate nohighlight">\(\mathbf A\)</span>.</p>
<p>Note that by construction, we have that <span class="math notranslate nohighlight">\(\left|\mathcal P(\Theta)\right| = \left|\Theta\right|\)</span>. That is, the two sets have the same number of elements, since since <span class="math notranslate nohighlight">\(\theta \in \Theta\)</span> has a particular distribution <span class="math notranslate nohighlight">\(\mathbb P_\theta \in \mathcal P(\Theta)\)</span>, and vice-versa. It is, in general, good for <span class="math notranslate nohighlight">\(\mathcal P(\Theta)\)</span> to be fairly rich; that is, when we specify a parametrized statistical model <span class="math notranslate nohighlight">\((\mathcal A_n, \mathcal P(\Theta))\)</span>, we want <span class="math notranslate nohighlight">\(\mathcal P(\Theta)\)</span> to contain distributions that we think faithfully could represent our network realization <span class="math notranslate nohighlight">\(A\)</span>. What is the most natural choice for <span class="math notranslate nohighlight">\(\mathcal P(\Theta)\)</span> that makes any sense?</p>
<p>If you are used to traditional univariate or multivariate statistical modelling, an extremely natural choice for when you have a discrete sample space (like <span class="math notranslate nohighlight">\(\mathcal A_n\)</span>, which is discrete because we can count it) would be to use a categorical model. In the categorical model, we would have a single parameter for all possible configurations of an <span class="math notranslate nohighlight">\(n\)</span>-node network; that is, <span class="math notranslate nohighlight">\(|\theta| = \left|\mathcal A_n\right| = 2^{n^2}\)</span>. What is wrong with this model? The limitations are two-fold:</p>
<ol class="simple">
<li><p>As we explained previously, when <span class="math notranslate nohighlight">\(n\)</span> is just <span class="math notranslate nohighlight">\(10\)</span>, we would need about <span class="math notranslate nohighlight">\(30,000,000\)</span> times the total number of storage available in the world to represent the parameters of a single distribution.</p></li>
<li><p>With a single network observed (or really, any number of networks we could collect in the real world) we would never be able to estimate <span class="math notranslate nohighlight">\(2^{n^2}\)</span> parameters. The number grows too quickly with <span class="math notranslate nohighlight">\(n\)</span> for any realistic choice of <span class="math notranslate nohighlight">\(n\)</span> in real-world data. This would lead to a thing called a <em>lack of identifiability</em> with a single network, which means that we would never be able to estimate <span class="math notranslate nohighlight">\(2^{n^2}\)</span> parameters from <span class="math notranslate nohighlight">\(1\)</span> network.</p></li>
</ol>
<p>So, what are some more reasonable descriptions of <span class="math notranslate nohighlight">\(\mathcal P\)</span>? We explore some choices below.</p>
<div class="section" id="erdos-renyi-er">
<h2>Erdös-Rényi (ER)<a class="headerlink" href="#erdos-renyi-er" title="Permalink to this headline">¶</a></h2>
<p>The simplest random network model is called the Erdös Rényi (ER) model<sup>1</sup>. Consider the social network example explained above. The simplest possible thing to do with our network would be to assume that a given pair of people within our network have the same chance of being friends as any other pair of people we select. The Erdös Rényi model formalizes this relatively simple model with a single parameter:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(p\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\([0, 1]\)</span></p></td>
<td><p>Probability that an edge exists between a pair of vertices</p></td>
</tr>
</tbody>
</table>
<p>In an Erdös Rényi network, each pair of nodes is connected with probability <span class="math notranslate nohighlight">\(p\)</span>, and therefore not connected with probability <span class="math notranslate nohighlight">\(1-p\)</span>. Statistically, we say that for each edge <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span>, that <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> is sampled independently and identically from a <span class="math notranslate nohighlight">\(Bernoulli(p)\)</span> distribution. The word “independent” means that edges in the network occurring or not occurring do not affect one another. For instance, this means that if we knew a student named Alice was friends with Bob, and Alice was also friends with Hakim, that we do not learn any information about whether Bob is friends with Hakim. The word “identical” means that every edge in the network has the same probability <span class="math notranslate nohighlight">\(p\)</span> of being connected. If Alice and Bob are friends with probability <span class="math notranslate nohighlight">\(p\)</span>, then Alice/Bob and Hakim are friends with probability <span class="math notranslate nohighlight">\(p\)</span>, too. If <span class="math notranslate nohighlight">\(\pmb A\)</span> is the adjacency matrix for an ER network with probability <span class="math notranslate nohighlight">\(p\)</span>, we write that <span class="math notranslate nohighlight">\(\pmb A \sim ER_n(p)\)</span>.</p>
<p>What is the likelihood for a realization <span class="math notranslate nohighlight">\(A\)</span> of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> which is <span class="math notranslate nohighlight">\(ER_n(p)\)</span>? Remember that the likelihood is proportional to the probability of observing a particular realization <span class="math notranslate nohighlight">\(A\)</span> of a random network <span class="math notranslate nohighlight">\(\mathbf A\)</span> given the parameters <span class="math notranslate nohighlight">\(\theta\)</span>. With <span class="math notranslate nohighlight">\(\theta = p\)</span> (that is, an ER-random network has only a single parameter, the probability <span class="math notranslate nohighlight">\(p\)</span>) we get:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal L(A; p) &amp;\propto \mathbb P(\mathbf A = A; p) \\
    &amp;= \prod_{i = 1}^n \prod_{j = 1}^n \mathbb P(\mathbf{a}_{ij} = a_{ij}; p)
\end{align*}\]</div>
<p>The above step follows by using the fact that the entries of <span class="math notranslate nohighlight">\(\pmb A\)</span> are independent, which means that the probability of the random network <span class="math notranslate nohighlight">\(\pmb A\)</span> taking the value <span class="math notranslate nohighlight">\(A\)</span> is the product of the probabilities of the individual random adjacencies <span class="math notranslate nohighlight">\(\mathbf{a}_{ij}\)</span> taking the values <span class="math notranslate nohighlight">\(a_{ij}\)</span>. Next, we recall that by assumption of the <span class="math notranslate nohighlight">\(ER_n(p)\)</span> model, that <span class="math notranslate nohighlight">\(\pmb a_{ij} \sim Bern(p)\)</span> for all <span class="math notranslate nohighlight">\(\pmb a_{ij}\)</span>, by the identical distribution assumption. Therefore, for each <span class="math notranslate nohighlight">\(\mathbb P(a_{ij}; p)\)</span>, it is the case that <span class="math notranslate nohighlight">\(\mathbb P(\mathbf{a_{ij}} = a_{ij}; p) = p^{a_{ij}}(1 - p)^{1 - a_{ij}}\)</span>, which is the probability mass function for a Bernoulli-distributed random variable. Therefore, using the shorthand <span class="math notranslate nohighlight">\(\sum_{i,j}\)</span> to mean the double sum <span class="math notranslate nohighlight">\(\sum_{i = 1}^n \sum_{j = 1}^n\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathcal L(A; p) &amp;\propto \prod_{i = 1}^n \prod_{j = 1}^n p^{a_{ij}}(1 - p)^{1 - a_{ij}} \\
    &amp;= p^{\sum_{i,j} a_{ij}} \cdot (1 - p)^{n - \sum_{i,j}a_{ij}} \\
    &amp;= p^{|\mathcal E|} \cdot (1 - p)^{n - |\mathcal E|}
\end{align*}\]</div>
<p>This means that the likelihood <span class="math notranslate nohighlight">\(\mathcal L(A; p)\)</span> is a function <em>only</em> of the number of edges <span class="math notranslate nohighlight">\(|\mathcal E|\)</span> in the network represented by adjacency matrix <span class="math notranslate nohighlight">\(A\)</span>. There is a big implication here: we can define <strong>equivalence classes</strong> of the sample space <span class="math notranslate nohighlight">\(\mathcal A_n\)</span> in terms of their likelihood. The equivalence class on the <span class="math notranslate nohighlight">\(ER_n(p)\)</span> random networks are the sets:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    E_{i} &amp;= \left\{A \in \mathcal A_n : \sum_{i,j}a_{ij} = i\right\}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> index from <span class="math notranslate nohighlight">\(0\)</span> (the minimum number of edges possible) all the way up to <span class="math notranslate nohighlight">\(n^2\)</span> (the maximum number of edges possible). To drive this intuition home a little bit more, what we mean here is that for any <span class="math notranslate nohighlight">\(A \in E_i\)</span> and any other <span class="math notranslate nohighlight">\(A'\)</span> also <span class="math notranslate nohighlight">\(\in E_i\)</span> (that is, both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A'\)</span> have the same number of edges, <span class="math notranslate nohighlight">\(i\)</span>), that <span class="math notranslate nohighlight">\(\mathcal L(A; p) = \mathcal A'; p)\)</span> for any choice <span class="math notranslate nohighlight">\(p \in [0,1]\)</span>. No matter the ER-random network (regardless of the choice of <span class="math notranslate nohighlight">\(p\)</span>) both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A'\)</span> will be equally likely because they have the same number of edges. Further, they <em>will</em> be more or less likely depending on the choice of <span class="math notranslate nohighlight">\(p\)</span> to networks which are in <em>other</em> equivalence classes. We can summarize these statements as follows. For any choice <span class="math notranslate nohighlight">\(p \in [0, 1]\)</span>:</p>
<ol class="simple">
<li><p>For any <span class="math notranslate nohighlight">\(A, A' \in E_i\)</span> (both <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(A'\)</span> are in the equivalence class of networks with <span class="math notranslate nohighlight">\(i\)</span> edges), <span class="math notranslate nohighlight">\(\mathcal L(A; p) = \mathcal L(A'; p)\)</span>, and</p></li>
<li><p>For any <span class="math notranslate nohighlight">\(A \in E_i\)</span> and <span class="math notranslate nohighlight">\(A'' \in E_j\)</span> (that is, <span class="math notranslate nohighlight">\(A\)</span> is in the equivalence class with <span class="math notranslate nohighlight">\(i\)</span> edges but <span class="math notranslate nohighlight">\(A''\)</span> is in the equivalence class with <span class="math notranslate nohighlight">\(j\)</span> edges), if <span class="math notranslate nohighlight">\(i \neq j\)</span>, then <span class="math notranslate nohighlight">\(\mathcal L(A; p) \neq \mathcal L(A''; p)\)</span>.</p></li>
</ol>
<p>In practice, the ER model seems like it might be a little too simple to be useful. Why would it ever be useful to think that the best we can do to describe our network is to say that connections exist with some probability? Does this miss a <em>lot</em> of useful questions we might want to answer? Fortunately, there are a number of ways in which the simplicity of the ER model is useful. Given a probability and a number of nodes, we can easily describe the properties we would expect to see in a network if that network were ER. For instance, we know what the degree distribution of an ER network should look like. We can reverse this idea, too: given a network we think might <em>not</em> be ER, we could check whether it’s different in some way from a network which is ER. For instance, if we see a half of the nodes have a very high degree, and the rest of the nodes with a much lower degree, we can reasonably conclude the network might be more complex than can be described by the ER model. If this is the case, we might look for other, more complex, models that could describe our network. In some of the exercises at the end of this section, we will explore how to formalize these relationships.</p>
<div class="admonition-working-out-the-expected-degree-in-an-erd-ouml-s-r-eacute-nyi-network admonition">
<p class="admonition-title">Working Out the Expected Degree in an Erdös-Rényi Network</p>
<p>Suppose that <span class="math notranslate nohighlight">\(A\)</span> is a simple network. The network has <span class="math notranslate nohighlight">\(n\)</span> nodes <span class="math notranslate nohighlight">\(\mathcal V = (v_i)_{i = 1}^n\)</span>. Recall that the in a simple network, the node degree is <span class="math notranslate nohighlight">\(deg(v_i) = \sum_{j = 1}^n A_{ij}\)</span>. What is the expected degree of a given node <span class="math notranslate nohighlight">\(v_i\)</span> if <span class="math notranslate nohighlight">\(A\)</span> were <span class="math notranslate nohighlight">\(ER_n(p)\)</span>?</p>
<p>To describe this, we will compute the expectated value of the degree <span class="math notranslate nohighlight">\(deg(v_i)\)</span>, written <span class="math notranslate nohighlight">\(\mathbb E\left[deg(v_i)\right]\)</span>. Let’s see what happens:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i)\right] &amp;= \mathbb E\left[\sum_{j = 1}^n A_{ij}\right] \\
    &amp;= \sum_{j = 1}^n \mathbb E[A_{ij}]
\end{align*}\]</div>
<p>We use the <em>linearity of expectation</em> in the line above, which means that the expectation of a sum with a finite number of terms being summed over (<span class="math notranslate nohighlight">\(n\)</span>, in this case) is the sum of the expectations. Finally, by definition, all of the edges <span class="math notranslate nohighlight">\(A_{ij}\)</span> have the same distribution: <span class="math notranslate nohighlight">\(Bernoulli(p)\)</span>. The expected value of a random quantity which takes a Bernoulli distribution is just the probability <span class="math notranslate nohighlight">\(p\)</span>! This means every term <span class="math notranslate nohighlight">\(\mathbb E[A_{ij}] = p\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb E\left[deg(v_i)\right] &amp;= \sum_{j = 1}^n p = n\cdot p
\end{align*}\]</div>
<p>Since all of the <span class="math notranslate nohighlight">\(n\)</span> terms being summed have the same expected value. This holds for <em>every</em> node <span class="math notranslate nohighlight">\(v_i\)</span>, which means that the expected degree of all nodes is an undirected ER network is the same.</p>
</div>
<!-- The ER model is also useful for the development of new computational techniques to use on random networks. This is because even if the "best" model for a network is something much more complex, we can still calculate an edge probability $p$ for the network without needing any information but the adjacency matrix. Consider, for instance, a case where we design a new algorithm for a social network, and we want to know how much more RAM we might need as the social network grows. We might want to investigate how the algorithm scales to networks with different numbers of people and different connection probabilities that might be realistic as our social network expands in popularity. Examining how the algorithm operates on ER networks with different values of $n$ and $p$ might be helpful. This is an especially common approach when people deal with networks that are said to be *sparse*. A **sparse network** is a network in which the number of edges is much less than the total possible number of edges. This contrasts with a **dense network**, which is a network in which the number of edges is close to the maximum number of possible edges. In the case of an $ER_{n}(p)$ network, the network is sparse when $p$ is small (closer to $0$), and dense when $p$ is large (closer to $1$). -->
<p>In the next code block, we look to sample a single ER network with <span class="math notranslate nohighlight">\(50\)</span> nodes and an edge probability <span class="math notranslate nohighlight">\(p\)</span> of <span class="math notranslate nohighlight">\(0.3\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">heatmap</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">er_np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># network with 50 nodes</span>
<span class="n">ps</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># probability of an edge existing is .3</span>

<span class="c1"># sample a single adj. mtx from ER(50, .3)</span>
<span class="n">As</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">ps</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">As</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$ER_</span><span class="si">{50}</span><span class="s2">(0.3)$ Simulation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;$ER_{50}(0.3)$ Simulation&#39;}&gt;
</pre></div>
</div>
<img alt="../../_images/single-network-models_2_1.png" src="../../_images/single-network-models_2_1.png" />
</div>
</div>
<p>Above, we visualize the network using a heatmap. The dark red squares indicate that an edge exists between a pair of vertices, and white squares indicate that an edge does not exist between a pair of vertices.</p>
<p>Next, let’s see what happens when we use a higher edge probability, like <span class="math notranslate nohighlight">\(p=0.7\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pl</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c1"># network has an edge probability of 0.7</span>

<span class="c1"># sample a single adj. mtx from ER(50, 0.7)</span>
<span class="n">Al</span> <span class="o">=</span> <span class="n">er_np</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pl</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">heatmap</span><span class="p">(</span><span class="n">Al</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$ER_</span><span class="si">{50}</span><span class="s2">(0.7)$ Simulation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;AxesSubplot:title={&#39;center&#39;:&#39;$ER_{50}(0.7)$ Simulation&#39;}&gt;
</pre></div>
</div>
<img alt="../../_images/single-network-models_5_1.png" src="../../_images/single-network-models_5_1.png" />
</div>
</div>
<p>As the edge probability increases, the sampled adjacency matrix tends to indicate that there are more connections in the network. This is because there is a higher chance of an edge existing when <span class="math notranslate nohighlight">\(p\)</span> is larger.</p>
</div>
<div class="section" id="stochastic-block-model-sbm">
<h2>Stochastic Block Model (SBM)<a class="headerlink" href="#stochastic-block-model-sbm" title="Permalink to this headline">¶</a></h2>
<p>Consider the social network above. Using the ER model described above, we can only model the connections between a pair of people with a <em>fixed probability</em> that is the same for all pairs of students within the network. What would happen if we had additional information about students in our network that might influence the probability that they were friends? Consider, for instance, that we know which school each student attends. It seems fairly logical that if two students go to the same school, in general, they will probably have a higher probability of being friends than if they went to different schools. Unfortunately, using the ER model, there is nothing more complex we can do to reflect this fact. This is where the Stochastic Block Model, or SBM, comes into play. The Stochastic Block Model generalizes the ER model, and has the following two parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vec\tau\)</span></p></td>
<td><p>{1,…,K}<span class="math notranslate nohighlight">\(^n\)</span></p></td>
<td><p>The community assignment vector for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes to one of <span class="math notranslate nohighlight">\(K\)</span> communities</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\pmb B\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{K \times K}\)</span></p></td>
<td><p>The symmetric block matrix, which assigns edge probabilities for pairs of communities</p></td>
</tr>
</tbody>
</table>
<p>With the SBM, we suppose that each node can be assigned to a group of nodes, called a <strong>community</strong>. Instead of describing all pairs of nodes by a fixed probability like with the ER model, in the SBM, we instead describe properties that hold for edges between <em>pairs of communities</em>. We write that <span class="math notranslate nohighlight">\(\vec \tau \in \{1, ..., K\}^n\)</span>, which means that <span class="math notranslate nohighlight">\(\vec \tau\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector which takes one of <span class="math notranslate nohighlight">\(K\)</span> possible values. In our social network example, for instance, this vector would reflect the fact that each of the <span class="math notranslate nohighlight">\(n\)</span> students in our network could be attendees at one of <span class="math notranslate nohighlight">\(K\)</span> possible schools. For a single node <span class="math notranslate nohighlight">\(i\)</span> that is in community <span class="math notranslate nohighlight">\(\ell\)</span>, where <span class="math notranslate nohighlight">\(\ell \in \{1, ..., K\}\)</span>, we write that <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span>.</p>
<p>Next, let’s discuss the matrix <span class="math notranslate nohighlight">\(\pmb B\)</span>, which is known as the <strong>block matrix</strong> of the SBM. We write down that <span class="math notranslate nohighlight">\(\pmb B \in [0, 1]^{K \times K}\)</span>, which means that the block matrix is a matrix with <span class="math notranslate nohighlight">\(K\)</span> rows and <span class="math notranslate nohighlight">\(K\)</span> columns. If we have a pair of nodes and know which of the <span class="math notranslate nohighlight">\(K\)</span> communities each node is from, the block matrix tells us the probability that those two nodes are connected. This matrix <span class="math notranslate nohighlight">\(\pmb B\)</span> is also symmetric, which means that if <span class="math notranslate nohighlight">\(b_{k, \ell} = p\)</span> where <span class="math notranslate nohighlight">\(p\)</span> is a probability, that <span class="math notranslate nohighlight">\(b_{\ell, k} = p\)</span>, too.</p>
<p>Finally, let’s think about how to write down the generative model for the SBM. Remember that <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(\pmb B\)</span> are parameters, which means that when we fit a SBM, we know these ahead of time. Intuitionally what we want to reflect is, if we know that node <span class="math notranslate nohighlight">\(i\)</span> is in community <span class="math notranslate nohighlight">\(\ell\)</span> and node <span class="math notranslate nohighlight">\(j\)</span> is in community <span class="math notranslate nohighlight">\(k\)</span>, that the <span class="math notranslate nohighlight">\((\ell, k)\)</span> entry of the block matrix is the probability that <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are connected. We write this statement down by saying that <span class="math notranslate nohighlight">\(A_{ij}\)</span> conditioned on <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bernoulli(b_{\ell, k})\)</span> distribution. Like the ER model, the edges are still independent, which means that edges occuring or not occuring do not have an impact on one another. Unlike above, we can no longer say that the <span class="math notranslate nohighlight">\(A_{ij}\)</span>s are identically distributed, since the probability <span class="math notranslate nohighlight">\(b_{\ell, k}\)</span> depends on the communities that nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are assigned to. This means that if we were to look at a different pair of nodes, the probability that the two nodes are connected could be different, and therefore they might not share the same distribution.</p>
<p>We just covered a lot of intuition that will be critical for understanding many of the later models, so let’s work through an example. Let’s assume that we have <span class="math notranslate nohighlight">\(300\)</span> students, and we know that each student goes to <span class="math notranslate nohighlight">\(1\)</span> of <span class="math notranslate nohighlight">\(2\)</span> possible schools. We don’t really care too much about the ordering of the students for now, so let’s just assume that the students are organized such that the first <span class="math notranslate nohighlight">\(150\)</span> students all go to school <span class="math notranslate nohighlight">\(1\)</span>, and the second <span class="math notranslate nohighlight">\(150\)</span> students all go to school <span class="math notranslate nohighlight">\(2\)</span>. Let’s assume that the students from school <span class="math notranslate nohighlight">\(1\)</span> are a little bit more closely knit than the students from school <span class="math notranslate nohighlight">\(2\)</span>, so we’ll say that the probability of two students who both go to school <span class="math notranslate nohighlight">\(1\)</span> being connected is <span class="math notranslate nohighlight">\(0.5\)</span>, and the probability of two students who both go to school <span class="math notranslate nohighlight">\(2\)</span> being friends is <span class="math notranslate nohighlight">\(0.3\)</span>. Finally, let’s assume that if one student goes to school <span class="math notranslate nohighlight">\(1\)</span> and the other student goes to school <span class="math notranslate nohighlight">\(2\)</span>, that the probability that they are friends is <span class="math notranslate nohighlight">\(0.2\)</span>.</p>
<div class="admonition-thought-exercise admonition">
<p class="admonition-title">Thought Exercise</p>
<p>Before you read on, try to think to yourself about what the community assignment vector <span class="math notranslate nohighlight">\(\vec \tau\)</span> and the block matrix <span class="math notranslate nohighlight">\(\pmb B\)</span> look like.</p>
</div>
<p>Next, let’s plot what <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(\pmb B\)</span> look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="k">def</span> <span class="nf">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Node&quot;</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;skyblue&quot;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">((</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">tau</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="o">.</span><span class="mi">75</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([</span><span class="s1">&#39;School 1&#39;</span><span class="p">,</span> <span class="s1">&#39;School 2&#39;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="n">xlab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mf">149.5</span><span class="p">,</span><span class="mf">299.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;150&quot;</span><span class="p">,</span> <span class="s2">&quot;300&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">300</span>  <span class="c1"># number of students</span>

<span class="c1"># tau is a column vector of 150 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 300 students are from</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Tau, Node Assignment Vector&quot;</span><span class="p">,</span>
        <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_8_0.png" src="../../_images/single-network-models_8_0.png" />
</div>
</div>
<p>So as we can see, the first <span class="math notranslate nohighlight">\(50\)</span> students are from school <span class="math notranslate nohighlight">\(1\)</span>, and the second <span class="math notranslate nohighlight">\(50\)</span> students are from school <span class="math notranslate nohighlight">\(2\)</span>. Next, let’s look at the block matrix <span class="math notranslate nohighlight">\(\pmb B\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 2 communities in total</span>
<span class="c1"># construct the block matrix B as described above</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="n">K</span><span class="p">))</span>
<span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>
<span class="n">B</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
<span class="n">B</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">.</span><span class="mi">3</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">heatmap</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;School&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;School&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;2&quot;</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span><span class="s2">&quot;2&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_10_0.png" src="../../_images/single-network-models_10_0.png" />
</div>
</div>
<p>As we can see, the matrix <span class="math notranslate nohighlight">\(\pmb B\)</span> is a symmetric block matrix. Finally, let’s sample a single network from the SBM with parameters <span class="math notranslate nohighlight">\(\vec \tau\)</span> and <span class="math notranslate nohighlight">\(\pmb B\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">sbm</span>
<span class="kn">from</span> <span class="nn">graspologic.plot</span> <span class="kn">import</span> <span class="n">adjplot</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># sample a graph from SBM_{300}(tau, B)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sbm</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">)],</span> <span class="n">p</span><span class="o">=</span><span class="n">B</span><span class="p">)</span>
<span class="n">meta</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;School&quot;</span><span class="p">:</span> <span class="n">tau</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)}</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">adjplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">meta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_12_0.png" src="../../_images/single-network-models_12_0.png" />
</div>
</div>
<p>The above network shows students, ordered by the school they are in (school 1 and school 2, respectively). As we can see in the above network, people from school <span class="math notranslate nohighlight">\(1\)</span> are more connected than people from school <span class="math notranslate nohighlight">\(2\)</span>.  We notice this from the fact that there are more connections between people from school <span class="math notranslate nohighlight">\(1\)</span> than from school <span class="math notranslate nohighlight">\(2\)</span>. Also, the connections between people from different schools appear to be a bit <em>more sparse</em> (fewer edges) than connections betwen schools. The above heatmap can be described as <strong>modular</strong>: it has clearly delineated communities, which are the vertices that comprise the obvious “squares” in the above adjacency matrix.</p>
<p>Something easy to mistake about the SBM is that the SBM will <em>not always</em> have the obvious modular structure defined above when we look at a heatmap. Rather, this modular structure is <em>only</em> made obvious because the students are ordered according to the school in which they are in. What do you think will happen if we look at the students in a random order? Do you think it will be obvious that the network will have a modular structure?</p>
<p>The answer is: <em>No!</em> Let’s see what happens when we use an reordering, called a <em>permutation</em> of the nodes, to reorder the nodes from the network into a random order:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># generate a permutation of the n vertices</span>
<span class="n">vtx_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">meta</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;School&quot;</span><span class="p">:</span> <span class="n">tau</span><span class="p">[</span><span class="n">vtx_perm</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">n</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)}</span>
<span class="p">)</span>

<span class="c1"># same adjacency matrix (up to reorder of the vertices)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">adjplot</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="nb">tuple</span><span class="p">([</span><span class="n">vtx_perm</span><span class="p">])]</span> <span class="p">[:,</span><span class="n">vtx_perm</span><span class="p">],</span> <span class="n">meta</span><span class="o">=</span><span class="n">meta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Blues&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_14_0.png" src="../../_images/single-network-models_14_0.png" />
</div>
</div>
<p>Notice that now, the students are <em>not</em> organized according to school. We can see this by looking at the school assignment vector, shown at the left and top, of the network. In this sense, it becomes quite difficult in practice to determine whether community structure exists simply by looking at a network, unless you are looking at a network in which the nodes are <em>already arranged</em> in an order which respects the community structure.</p>
<p>In practice, this means that if you know ahead of time what natural groupings of the nodes might be (such knowing which school each student goes to) by way of your node metadata, you <em>should</em> visualize your data according to that grouping. If you don’t know anything about natural groupings of nodes, however, we are left with the problem of <em>estimating community structure</em>. A later method, called the <em>spectral embedding</em>, will be paired with clustering techniques to allow us to estimate vertex assignment vectors.</p>
</div>
<div class="section" id="random-dot-product-graph-rdpg">
<h2>Random Dot Product Graph (RDPG)<a class="headerlink" href="#random-dot-product-graph-rdpg" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="generalized-random-dot-product-graph-grdpg">
<h2>Generalized Random Dot Product Graph (GRDPG)<a class="headerlink" href="#generalized-random-dot-product-graph-grdpg" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="degree-corrected-models">
<h2>Degree-Corrected Models<a class="headerlink" href="#degree-corrected-models" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="structured-independent-edge-model-siem">
<h2>Structured Independent Edge Model (SIEM)<a class="headerlink" href="#structured-independent-edge-model-siem" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="inhomogeneous-erdos-renyi-ier">
<h2>Inhomogeneous Erdos-Renyi (IER)<a class="headerlink" href="#inhomogeneous-erdos-renyi-ier" title="Permalink to this headline">¶</a></h2>
</div>
</div>
<div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<p>[1] Erdös P, Rényi A. 1959. “On random graphs, I.” Publ. Math. Debrecen 6:290–297.</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./representations/ch5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="why-use-models.html" title="previous page">Why Use Statistical Models?</a>
    <a class='right-next' id="next-link" href="multi-network-models.html" title="next page">Multi-Network Models</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Joshua Vogelstein, Alex Loftus, and Eric Bridgeford<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>