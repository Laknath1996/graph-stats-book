
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5.5. Random Dot Product Graphs (RDPG) &#8212; Hands-on Network Machine Learning with Scikit-Learn and Graspologic</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="5.6. Multi-Network Models" href="multi-network-models.html" />
    <link rel="prev" title="5.4. Stochastic Block Models (SBM)" href="single-network-models_SBM.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Hands-on Network Machine Learning with Scikit-Learn and Graspologic</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../coverpage.html">
   Hands-on Network Machine Learning with Scikit-Learn and Graspologic
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/preface.html">
   Preface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../introduction/terminology.html">
   Terminology and Math Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Foundations
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch1/ch1.html">
   1. The Network Machine Learning Landscape
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/what-is-a-network.html">
     1.1. What Is A Network?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/why-study-networks.html">
     1.2. Why Study Networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/examples-of-applications.html">
     1.3. Examples of applications
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-networks.html">
     1.4. Types of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/types-of-learning-probs.html">
     1.5. Types of Network Learning Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/main-challenges.html">
     1.6. Main Challenges of Network Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch1/exercises.html">
     1.7. Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch2/ch2.html">
   2. End-to-end Biology Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/big-picture.html">
     2.1. Look at the big picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/get-the-data.html">
     2.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/prepare-the-data.html">
     2.3. Prepare the Data for Network Algorithms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/transformation-techniques.html">
     2.4. Transformation Techniques
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/select-and-train.html">
     2.5. Select and Train a Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch2/fine-tune.html">
     2.6. Fine-Tune your Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../foundations/ch3/ch3.html">
   3. End-to-end Business Network Machine Learning Project
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/big-picture.html">
     3.1. Look at the Big Picture
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/get-the-data.html">
     3.2. Get the Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/discover-and-visualize.html">
     3.3. Discover and Visualize the Data to Gain Insights
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../foundations/ch3/prepare-the-data.html">
     3.4. Prepare the Data for Network Algorithms
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Representations
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/ch4.html">
   4. Properties of Networks as a Statistical Object
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/matrix-representations.html">
     4.1. Matrix Representations Of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/network-representations.html">
     4.2. Representations of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/properties-of-networks.html">
     4.3. Properties of Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/regularization.html">
     4.4. Regularization
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="ch5.html">
   5. Why Use Statistical Models?
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="why-use-models.html">
     5.1. Why Use Statistical Models?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single-network-models.html">
     5.2. Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single-network-models_ER.html">
     5.3. Erdös-Rényi (ER) Random Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="single-network-models_SBM.html">
     5.4. Stochastic Block Models (SBM)
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5.5. Random Dot Product Graphs (RDPG)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multi-network-models.html">
     5.6. Multi-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="models-with-covariates.html">
     5.7. Network Models with Covariates
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch6/ch6.html">
   6. Learning Network Representations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters_mle.html">
     6.1. Estimating Parameters in Network Models via MLE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/why-embed-networks.html">
     6.2. Why embed networks?
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/spectral-embedding.html">
     6.3. Spectral Embedding Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/estimating-parameters_spectral.html">
     6.4. Estimating Parameters in Network Models via Spectral Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/random-walk-diffusion-methods.html">
     6.5. Random-Walk and Diffusion-based Methods
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/graph-neural-networks.html">
     6.6. Graph Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/multigraph-representation-learning.html">
     6.7. Multiple-Network Representation Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/joint-representation-learning.html">
     6.8. Joint Representation Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch7/ch7.html">
   7. Theoretical Results
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-single-network.html">
     7.1. Theory for Single Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-multigraph.html">
     7.2. Theory for Multiple-Network Models
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/theory-matching.html">
     7.3. Theory for Graph Matching
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Applications
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch8/ch8.html">
   8. Leveraging Representations for Single Graph Applications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/community-detection.html">
     8.1. Community Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/testing-differences.html">
     8.2. Testing for Differences between Communities
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/model-selection.html">
     8.3. Model Selection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/vertex-nomination.html">
     8.4. Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/anomaly-detection.html">
     8.5. Anomaly Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch8/out-of-sample.html">
     8.6. Out-of-sample Embedding
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch9/ch9.html">
   9. Leveraging Representations for Multiple Graph Applications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/two-sample-hypothesis.html">
     9.1. Two-Sample Hypothesis Testing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/graph-matching-vertex.html">
     9.2. Graph Matching and Vertex Nomination
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch9/vertex-nomination.html">
     9.3. Vertex Nomination
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../applications/ch10/ch10.html">
   10. Algorithms for more than 2 graphs
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/anomaly-detection.html">
     10.1. Anomaly Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-edges.html">
     10.2. Testing for Significant Edges
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-vertices.html">
     10.3. Testing for Significant Vertices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../applications/ch10/significant-communities.html">
     10.4. Testing for Significant Communities
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://graspologic.readthedocs.io/en/latest/">
   Graspologic Documentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/representations/ch5/single-network-models_RDPG.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        
        
        <a class="edit-button" href="https://github.com/neurodata/graph-stats-book/edit/master/representations/ch5/single-network-models_RDPG.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/neurodata/graph-stats-book/master?urlpath=tree/representations/ch5/single-network-models_RDPG.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#latent-positions">
   5.5.1. Latent Positions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-priori-rdpg">
   5.5.2.
   <em>
    A Priori
   </em>
   RDPG
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#code-examples">
     5.5.2.1. Code Examples
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#probability">
     5.5.2.2. Probability*
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-posteriori-rdpg">
   5.5.3.
   <em>
    A Posteriori
   </em>
   RDPG
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     5.5.3.1. Probability*
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-random-dot-product-graph-grdpg">
   5.5.4. Generalized Random Dot Product Graph (GRDPG)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-priori-grdpg">
     5.5.4.1.
     <em>
      A Priori
     </em>
     GRDPG
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-posteriori-grdpg">
     5.5.4.2.
     <em>
      A Posteriori
     </em>
     GRDPG
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inhomogeneous-erdos-renyi-ier">
   5.5.5. Inhomogeneous Erdös-Rényi (IER)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     5.5.5.1. Probability*
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="random-dot-product-graphs-rdpg">
<h1><span class="section-number">5.5. </span>Random Dot Product Graphs (RDPG)<a class="headerlink" href="#random-dot-product-graphs-rdpg" title="Permalink to this headline">¶</a></h1>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># the number of nodes in our network</span>

<span class="c1"># design the latent position matrix X according to </span>
<span class="c1"># the rules we laid out previously</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">n</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">n</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;RDPG Simulation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_1_0.png" src="../../_images/single-network-models_RDPG_1_0.png" />
</div>
</div>
<div class="section" id="latent-positions">
<h2><span class="section-number">5.5.1. </span>Latent Positions<a class="headerlink" href="#latent-positions" title="Permalink to this headline">¶</a></h2>
<p>Let’s imagine that we have a network which follows the <em>a priori</em> Stochastic Block Model. To make this example a little bit more concrete, let’s borrow the code example from above. The nodes of our network represent each of the <span class="math notranslate nohighlight">\(100\)</span> students in our network. The node assignment vector represents which of the two schools eaach student attends, where the first <span class="math notranslate nohighlight">\(50\)</span> students attend the first school, and the second <span class="math notranslate nohighlight">\(50\)</span> students attend school <span class="math notranslate nohighlight">\(2\)</span>. Remember that <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> look like:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>

<span class="k">def</span> <span class="nf">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Node&quot;</span><span class="p">):</span>
    <span class="n">cmap</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">ListedColormap</span><span class="p">([</span><span class="s2">&quot;skyblue&quot;</span><span class="p">,</span> <span class="s1">&#39;blue&#39;</span><span class="p">])</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">((</span><span class="n">tau</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="n">tau</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticks</span><span class="p">([</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">.75</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([</span><span class="s1">&#39;School 1&#39;</span><span class="p">,</span> <span class="s1">&#39;School 2&#39;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="n">xlab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">.5</span><span class="p">,</span><span class="mf">49.5</span><span class="p">,</span><span class="mf">99.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;50&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># number of students</span>

<span class="c1"># tau is a column vector of 150 1s followed by 50 2s</span>
<span class="c1"># this vector gives the school each of the 300 students are from</span>
<span class="n">tau</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">)))</span>

<span class="n">plot_tau</span><span class="p">(</span><span class="n">tau</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Tau, Node Assignment Vector&quot;</span><span class="p">,</span>
        <span class="n">xlab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_3_0.png" src="../../_images/single-network-models_RDPG_3_0.png" />
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>  <span class="c1"># 2 communities in total</span>
<span class="c1"># construct the block matrix B as described above</span>
<span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]</span>

<span class="k">def</span> <span class="nf">plot_block</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">blockname</span><span class="o">=</span><span class="s2">&quot;School&quot;</span><span class="p">,</span> <span class="n">blocktix</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span>
               <span class="n">blocklabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;School 1&quot;</span><span class="p">,</span> <span class="s2">&quot;School 2&quot;</span><span class="p">]):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">blockname</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">blocktix</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">blocklabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_block</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Block Matrix&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_4_0.png" src="../../_images/single-network-models_RDPG_4_0.png" />
</div>
</div>
<p>Are there any other ways to describe this scenario, other than using both <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>?</p>
<p>What if we were to look at the probabilities for <em>every</em> pair of edges? Remember, for a given <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, that a network which is SBM can be generated using the approach that, given that <span class="math notranslate nohighlight">\(\tau_i = \ell\)</span> and <span class="math notranslate nohighlight">\(\tau_j = k\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(b_{\ell k})\)</span>. That is, every entry is Bernoulli, with the probability indicated by appropriate entry of the block matrix corresponding to the pair of communities each node is in. However, there’s another way we could write down this generative model. Suppose we had a <span class="math notranslate nohighlight">\(n \times n\)</span> probability matrix, where for every <span class="math notranslate nohighlight">\(j &gt; i\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    p_{ji} = p_{ij}, p_{ij} = \begin{cases}
        b_{11} &amp; \tau_i = 1, \tau_j = 1 \\
        b_{12} &amp; \tau_i = 1, \tau_j = 2 \\
        b_{22} &amp; \tau_i = 2, \tau_j = 1
    \end{cases}
\end{align*}\]</div>
<p>This matrix <span class="math notranslate nohighlight">\(P\)</span> with entries <span class="math notranslate nohighlight">\(p_{ij}\)</span> is the probability matrix associated with the <em>a priori</em> SBM, which we described in the section <a class="reference internal" href="single-network-models.html#representations-whyuse-networkmodels-iern"><span class="std std-ref">Independent-Edge Random Networks*</span></a>.
If you’ve been following the advanced sections, you will already be familiar with this term. Simply put, this matrix describes the probability of each edge <span class="math notranslate nohighlight">\((i,j)\)</span> existing. What does <span class="math notranslate nohighlight">\(P\)</span> look like?</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_prob</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">nodename</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">nodelabs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">nodename</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">nodetix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">nodelabs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">nodetix</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">nodelabs</span><span class="p">)</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mf">.5</span>
<span class="n">P</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="mf">.3</span>
<span class="n">P</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span> <span class="o">=</span> <span class="mf">.2</span>
<span class="n">P</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">50</span><span class="p">]</span> <span class="o">=</span> <span class="mf">.2</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plot_prob</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix&quot;</span><span class="p">,</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span>
              <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_6_0.png" src="../../_images/single-network-models_RDPG_6_0.png" />
</div>
</div>
<p>As we can see, <span class="math notranslate nohighlight">\(P\)</span> captures a similar modular structure to the actual adjacency matrix corresponding to the SBM network. When we say this network is <em>modular</em>, we mean that it looks block-y, in that there are clusters of edges sharing a similar probability. Also, <span class="math notranslate nohighlight">\(P\)</span> captures the probability of connections between each pair of students. Indeed, it is the case that <span class="math notranslate nohighlight">\(P\)</span> contains the information of both <span class="math notranslate nohighlight">\(\vec\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span>. This means that we can write down a generative model by specifying <em>only</em> <span class="math notranslate nohighlight">\(P\)</span>, and we no longer need to specify <span class="math notranslate nohighlight">\(\vec\tau\)</span> and <span class="math notranslate nohighlight">\(B\)</span> at all. To write down the generative model in this way, we say that for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>, that <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(p_{ij})\)</span> independently, where <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>.</p>
<p>What is so special about this formulation of the SBM problem? As it turns out, for a <em>positive semi-definite</em> probability matrix <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(P\)</span> can be decomposed using a matrix <span class="math notranslate nohighlight">\(X\)</span>, where <span class="math notranslate nohighlight">\(P = X X^\top\)</span>. We will call a single row of <span class="math notranslate nohighlight">\(X\)</span> the vector <span class="math notranslate nohighlight">\(\vec x_i\)</span>. Remember, using this expression, each entry <span class="math notranslate nohighlight">\(p_{ij}\)</span> is the product <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span>, for all <span class="math notranslate nohighlight">\(i, j\)</span>. Like <span class="math notranslate nohighlight">\(P\)</span>, <span class="math notranslate nohighlight">\(X\)</span> has <span class="math notranslate nohighlight">\(n\)</span> rows, each of which corresponds to a single node in our network. However, the special property of <span class="math notranslate nohighlight">\(X\)</span> is that it doesn’t <em>necessarily</em> have <span class="math notranslate nohighlight">\(n\)</span> columns: rather, <span class="math notranslate nohighlight">\(X\)</span> often will have many fewer columns than rows. For instance, with <span class="math notranslate nohighlight">\(P\)</span> as above, there in fact exists an <span class="math notranslate nohighlight">\(X\)</span> with just <span class="math notranslate nohighlight">\(2\)</span> columns that can be used to describe <span class="math notranslate nohighlight">\(P\)</span>. This matrix <span class="math notranslate nohighlight">\(X\)</span> will be called the <strong>latent position matrix</strong>, and each row <span class="math notranslate nohighlight">\(\vec x_i\)</span> will be called the <strong>latent position of a node</strong>.</p>
<p>Now, your next thought might be that this requires a <em>lot</em> more space to represent an SBM network, and you’d be right: <span class="math notranslate nohighlight">\(\vec \tau\)</span> has <span class="math notranslate nohighlight">\(n\)</span> entries, and <span class="math notranslate nohighlight">\(B\)</span> has <span class="math notranslate nohighlight">\(K \times K\)</span> entries, where <span class="math notranslate nohighlight">\(K\)</span> is typically much smaller than <span class="math notranslate nohighlight">\(n\)</span>. On the other hand, in this formulation, <span class="math notranslate nohighlight">\(P\)</span> has <span class="math notranslate nohighlight">\(\binom{n}{2}\)</span> entries, which is much bigger than <span class="math notranslate nohighlight">\(n + K \times K\)</span> (since <span class="math notranslate nohighlight">\(K\)</span> is usually much smaller than <span class="math notranslate nohighlight">\(n\)</span>). The advantage is that under this formulation, <span class="math notranslate nohighlight">\(P\)</span> doesn’t need to have this rigorous modular structure characteristic of SBM networks, and can look a <em>lot</em> more interesting. As we will see in later chapters, this network representation will prove extremely flexible for allowing us to capture networks that are fairly complex. Further, we can also perform analysis on the matrix <span class="math notranslate nohighlight">\(X\)</span> itself, which will prove very useful for estimation of SBMs.</p>
</div>
<div class="section" id="a-priori-rdpg">
<h2><span class="section-number">5.5.2. </span><em>A Priori</em> RDPG<a class="headerlink" href="#a-priori-rdpg" title="Permalink to this headline">¶</a></h2>
<p>The <em>a priori</em> Random Dot Product Graph is an RDPG in which we know <em>a priori</em> the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>. The <em>a priori</em> RDPG has the following parameter:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(X\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \mathbb R^{n \times d}\)</span></p></td>
<td><p>The matrix of latent positions for each node <span class="math notranslate nohighlight">\(n\)</span>.</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(X\)</span> is called the <strong>latent position matrix</strong> of the RDPG. We write that <span class="math notranslate nohighlight">\(X \in \mathbb R^{n \times d}\)</span>, which means that it is a matrix with real values, <span class="math notranslate nohighlight">\(n\)</span> rows, and <span class="math notranslate nohighlight">\(d\)</span> columns. We will use the notation <span class="math notranslate nohighlight">\(\vec x_i\)</span> to refer to the <span class="math notranslate nohighlight">\(i^{th}\)</span> row of <span class="math notranslate nohighlight">\(X\)</span>. <span class="math notranslate nohighlight">\(\vec x_i\)</span> is referred to as the <strong>latent position</strong> of a node <span class="math notranslate nohighlight">\(i\)</span>. This looks something like this:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    X = \begin{bmatrix}
     \vec x_{1}^\top \\
     \vdots \\
     \vec x_n^\top
    \end{bmatrix}
\end{align*}\]</div>
<p>Noting that <span class="math notranslate nohighlight">\(X\)</span> has <span class="math notranslate nohighlight">\(d\)</span> columns, this implies that <span class="math notranslate nohighlight">\(\vec x_i \in  \mathbb R^d\)</span>, or that each node’s latent position is a real-valued <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector.</p>
<p>What is the generative model for the <em>a priori</em> RDPG? As we discussed above, given <span class="math notranslate nohighlight">\(X\)</span>, for all <span class="math notranslate nohighlight">\(j &gt; i\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(\vec x_i^\top \vec x_j)\)</span> independently. If <span class="math notranslate nohighlight">\(i &lt; j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (the network is <em>undirected</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> (the network is <em>loopless</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> RDPG with parameter <span class="math notranslate nohighlight">\(X\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim RDPG_n(X)\)</span>.</p>
<div class="section" id="code-examples">
<h3><span class="section-number">5.5.2.1. </span>Code Examples<a class="headerlink" href="#code-examples" title="Permalink to this headline">¶</a></h3>
<p>We will let <span class="math notranslate nohighlight">\(X\)</span> be a little more complex than in our preceding example. Our <span class="math notranslate nohighlight">\(X\)</span> will produce a <span class="math notranslate nohighlight">\(P\)</span> that still <em>somewhat</em> has a modular structure, but not quite as much as before. Let’s assume that we have <span class="math notranslate nohighlight">\(100\)</span> people who live along a very long road that is <span class="math notranslate nohighlight">\(100\)</span> miles long, and each person is <span class="math notranslate nohighlight">\(1\)</span> mile apart. The nodes of our network represent the people who live along our assumed street. If two people are closer to one another, it might make sense to think that they have a higher probability of being friends. If two people are neighbors, we think that they will have a very high probability of being friends (almost <span class="math notranslate nohighlight">\(1\)</span>) and when people are very far apart, we think that they will have a very low probability of being friends (almost <span class="math notranslate nohighlight">\(0\)</span>). What could we use for <span class="math notranslate nohighlight">\(X\)</span>?</p>
<p>Remember that the latent positions for each node <span class="math notranslate nohighlight">\(i\)</span> are denoted by the vector <span class="math notranslate nohighlight">\(\vec x_i\)</span>. One possible approach would be to let each <span class="math notranslate nohighlight">\(\vec x_i\)</span> be defined as follows:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vec x_i = \begin{bmatrix}
        \frac{100 - i}{100} \\
        \frac{i}{100}
    \end{bmatrix}
\end{align*}\]</div>
<p>For instance, <span class="math notranslate nohighlight">\(\vec x_1 = \begin{bmatrix}1 \\ 0\end{bmatrix}\)</span>, and <span class="math notranslate nohighlight">\(\vec x_{100} = \begin{bmatrix} 0 \\ 1\end{bmatrix}\)</span>. Note that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{1,100} = \vec x_1^\top \vec x_j = 1 \cdot 0 + 0 \cdot 1 = 0
\end{align*}\]</div>
<p>What happens in between?</p>
<p>Let’s consider another person, person <span class="math notranslate nohighlight">\(30\)</span>. Note that person <span class="math notranslate nohighlight">\(30\)</span> lives closer to person <span class="math notranslate nohighlight">\(1\)</span> than to person <span class="math notranslate nohighlight">\(100\)</span>.  Here, <span class="math notranslate nohighlight">\(\vec x_{30} = \begin{bmatrix} \frac{7}{10}\\ \frac{3}{10}\end{bmatrix}\)</span>. This gives us that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_{1,30} &amp;= \vec x_1^\top \vec x_{30} = \frac{7}{10}\cdot 1 + 0 \cdot \frac{3}{10} = \frac{7}{10} \\
p_{30, 100} &amp;= \vec x_{30}^\top x_{100} = \frac{7}{10} \cdot 0 + \frac{3}{10} \cdot 1 = \frac{3}{10}
\end{align*}\]</div>
<p>So this means that person <span class="math notranslate nohighlight">\(1\)</span> and person <span class="math notranslate nohighlight">\(30\)</span> have a <span class="math notranslate nohighlight">\(70\%\)</span> probability of being friends, but person <span class="math notranslate nohighlight">\(30\)</span> and <span class="math notranslate nohighlight">\(100\)</span> have onl6 a <span class="math notranslate nohighlight">\(30\%\)</span> probability of being friends.</p>
<p>Intuitively, it seems like our probability matrix <span class="math notranslate nohighlight">\(P\)</span> will capture the intuitive idea we described above. First, we’ll take a look at <span class="math notranslate nohighlight">\(X\)</span>, and then we’ll look at <span class="math notranslate nohighlight">\(P\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># the number of nodes in our network</span>

<span class="c1"># design the latent position matrix X according to </span>
<span class="c1"># the rules we laid out previously</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">[(</span><span class="n">n</span> <span class="o">-</span> <span class="n">i</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="o">/</span><span class="n">n</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_lp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">ylab</span><span class="o">=</span><span class="s2">&quot;Student&quot;</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    
    <span class="k">with</span> <span class="n">sns</span><span class="o">.</span><span class="n">plotting_context</span><span class="p">(</span><span class="s2">&quot;talk&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;Purples&quot;</span><span class="p">,</span>
                        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">yticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">xticklabels</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
        <span class="n">cbar</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">collections</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">colorbar</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="n">ylab</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">99</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;30&quot;</span><span class="p">,</span> <span class="s2">&quot;70&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Dimension 2&quot;</span><span class="p">])</span>
        <span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_frame_on</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span>

<span class="n">plot_lp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix, X&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_10_0.png" src="../../_images/single-network-models_RDPG_10_0.png" />
</div>
</div>
<p>The latent position matrix <span class="math notranslate nohighlight">\(X\)</span> that we plotted above is <span class="math notranslate nohighlight">\(n \times d\)</span> dimensions. There are a number of approaches, other than looking at a heatmap of <span class="math notranslate nohighlight">\(X\)</span>, with which we can visualize <span class="math notranslate nohighlight">\(X\)</span> to derive insights as to its structure. When <span class="math notranslate nohighlight">\(d=2\)</span>, another popular visualization is to look at the latent positions, <span class="math notranslate nohighlight">\(\vec x_i\)</span>, as individual points in <span class="math notranslate nohighlight">\(2\)</span>-dimensional space. This will give us a scatter plot of <span class="math notranslate nohighlight">\(n\)</span> points, each of which has two coordinates. Each point is the latent position for a single node:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_latents</span><span class="p">(</span><span class="n">latent_positions</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ss</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">latent_positions</span><span class="p">[</span><span class="n">ss</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">latent_positions</span><span class="p">[</span><span class="n">ss</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
                           <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Set1&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Dimension 1&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Dimension 2&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plot</span>

<span class="c1"># plot</span>
<span class="n">plot_latents</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Latent Position Matrix, X&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_12_0.png" src="../../_images/single-network-models_RDPG_12_0.png" />
</div>
</div>
<p>The above scatter plot has been subsampled to show only every <span class="math notranslate nohighlight">\(2^{nd}\)</span> latent position, so that the individual <span class="math notranslate nohighlight">\(2\)</span>-dimensional latent positions are discernable. Due to the way we constructed <span class="math notranslate nohighlight">\(X\)</span>, the scatter plot would otherwise appear to be a line (due to points overlapping one another). The reason that the points fall along a vertical line when plotted as a vector is due to the method we used to construct entries of <span class="math notranslate nohighlight">\(X\)</span>, described above. Next, we will look at the probability matrix:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_prob</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">()),</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Probability Matrix, P=$XX^T$&quot;</span><span class="p">,</span>
         <span class="n">nodelabs</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;1&quot;</span><span class="p">,</span> <span class="s2">&quot;30&quot;</span><span class="p">,</span> <span class="s2">&quot;70&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">],</span> <span class="n">nodetix</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">29</span><span class="p">,</span><span class="mi">69</span><span class="p">,</span><span class="mi">99</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_14_0.png" src="../../_images/single-network-models_RDPG_14_0.png" />
</div>
</div>
<p>Finally, we will sample an RDPG:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graspologic.simulations</span> <span class="kn">import</span> <span class="n">rdpg</span>
<span class="kn">from</span> <span class="nn">graphbook_code</span> <span class="kn">import</span> <span class="n">draw_multiplot</span>

<span class="c1"># sample an RDPG with the latent position matrix</span>
<span class="c1"># created above</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">rdpg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">directed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># and plot it</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">draw_multiplot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;$RDPG_</span><span class="si">{100}</span><span class="s2">(X)$ Simulation&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_16_0.png" src="../../_images/single-network-models_RDPG_16_0.png" />
</div>
</div>
</div>
<div class="section" id="probability">
<h3><span class="section-number">5.5.2.2. </span>Probability*<a class="headerlink" href="#probability" title="Permalink to this headline">¶</a></h3>
<p>Given <span class="math notranslate nohighlight">\(X\)</span>, the probability for an RDPG is relatively straightforward, as an RDPG is another Independent-Edge Random Graph. The independence assumption vastly simplifies our resulting expression. We will also use many of the results we’ve identified above, such as the p.m.f. of a Bernoulli random variable. Finally, we’ll note that the probability matrix <span class="math notranslate nohighlight">\(P = (\vec x_i^\top \vec x_j)\)</span>, so <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec x_j\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \mathbb P_\theta(A) \\
    &amp;= \prod_{j &gt; i}\mathbb P(\mathbf a_{ij} = a_{ij}),\;\;\;\; \textrm{Independence Assumption} \\
    &amp;= \prod_{j &gt; i}(\vec x_i^\top \vec x_j)^{a_{ij}}(1 - \vec x_i^\top \vec x_j)^{1 - a_{ij}},\;\;\;\; a_{ij} \sim Bern(\vec x_i^\top \vec x_j)
\end{align*}\]</div>
<p>Unfortunately, the probability equivalence classes are a bit harder to understand intuitionally here compared to the ER and SBM examples so we won’t write them down here, but they still exist!</p>
</div>
</div>
<div class="section" id="a-posteriori-rdpg">
<h2><span class="section-number">5.5.3. </span><em>A Posteriori</em> RDPG<a class="headerlink" href="#a-posteriori-rdpg" title="Permalink to this headline">¶</a></h2>
<p>Like for the <em>a posteriori</em> SBM, the <em>a posteriori</em> RDPG introduces another strange set: the <strong>intersection of the unit ball and the non-negative orthant</strong>. Huh? This sounds like a real mouthful, but it turns out to be rather straightforward. You are probably already very familiar with a particular orthant: in two-dimensions, an orthant is called a quadrant. Basically, an orthant just extends the concept of a quadrant to spaces which might have more than <span class="math notranslate nohighlight">\(2\)</span> dimensions. The non-negative orthant happens to be the orthant where all of the entries are non-negative. We call the <strong><span class="math notranslate nohighlight">\(K\)</span>-dimensional non-negative orthant</strong> the set of points in <span class="math notranslate nohighlight">\(K\)</span>-dimensional real space, where:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \left\{\vec x \in \mathbb R^K : x_k \geq 0\text{ for all $k$}\right\}
\end{align*}\]</div>
<p>In two dimensions, this is the traditional upper-right portion of the standard coordinate axis. To give you a picture, the <span class="math notranslate nohighlight">\(2\)</span>-dimensional non-negative orthant is the blue region of the following figure:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.axisartist</span> <span class="kn">import</span> <span class="n">SubplotZero</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patch</span>

<span class="k">class</span> <span class="nc">myAxes</span><span class="p">():</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xlim</span> <span class="o">=</span> <span class="n">xlim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span> <span class="o">=</span> <span class="n">ylim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">figsize</span>  <span class="o">=</span> <span class="n">figsize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__scale_arrows</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">__drawArrow</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> 
            <span class="n">color</span>       <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>
            <span class="n">clip_on</span>     <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> 
            <span class="n">head_width</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span><span class="p">,</span> 
            <span class="n">head_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_length</span>
        <span class="p">)</span> 
        
    <span class="k">def</span> <span class="nf">__scale_arrows</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Make the arrows look good regardless of the axis limits &quot;&quot;&quot;</span>
        <span class="n">xrange</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">yrange</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">head_width</span>  <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">xrange</span><span class="o">/</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_length</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">yrange</span><span class="o">/</span><span class="mi">30</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__drawAxis</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Draws the 2D cartesian axis</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># A subplot with two additional axis, &quot;xzero&quot; and &quot;yzero&quot;</span>
        <span class="c1"># corresponding to the cartesian axis</span>
        <span class="n">ax</span> <span class="o">=</span> <span class="n">SubplotZero</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
        
        <span class="c1"># make xzero axis (horizontal axis line through y=0) visible.</span>
        <span class="k">for</span> <span class="n">axis</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;xzero&quot;</span><span class="p">,</span><span class="s2">&quot;yzero&quot;</span><span class="p">]:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="n">axis</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># make the other axis (left, bottom, top, right) invisible</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">,</span> <span class="s2">&quot;right&quot;</span><span class="p">,</span> <span class="s2">&quot;bottom&quot;</span><span class="p">,</span> <span class="s2">&quot;top&quot;</span><span class="p">]:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
            
        <span class="c1"># Plot limits</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">])</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="c1"># Draw the arrows</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__drawArrow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xlim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span> <span class="c1"># x-axis arrow</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__drawArrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ylim</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span> <span class="c1"># y-axis arrow</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span>
        
    <span class="k">def</span> <span class="nf">draw</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># First draw the axis</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">figsize</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__drawAxis</span><span class="p">()</span>

<span class="n">axes</span> <span class="o">=</span> <span class="n">myAxes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

<span class="n">rectangle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.2</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rectangle</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_19_0.png" src="../../_images/single-network-models_RDPG_19_0.png" />
</div>
</div>
<p>Now, what is the unit ball? You are probably familiar with the idea of the unit ball, even if you haven’t heard it called that specifically. Remember that the Euclidean norm for a point <span class="math notranslate nohighlight">\(\vec x\)</span> which has coordinates <span class="math notranslate nohighlight">\(x_i\)</span> for <span class="math notranslate nohighlight">\(i=1,...,K\)</span> is given by the expression:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \left|\left|\vec x\right|\right|_2 = \sqrt{\sum_{i = 1}^K x_i^2}
\end{align*}\]</div>
<p>The Euclidean unit ball is just the set of points whose Euclidean norm is at most <span class="math notranslate nohighlight">\(1\)</span>. To be more specific, the <strong>closed unit ball</strong> with the Euclidean norm is the set of points:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \left\{\vec x \in \mathbb R^K :\left|\left|\vec x\right|\right|_2 \leq 1\right\}
\end{align*}\]</div>
<p>We draw the <span class="math notranslate nohighlight">\(2\)</span>-dimensional unit ball with the Euclidean norm below, where the points that make up the unit ball are shown in red:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axes</span> <span class="o">=</span> <span class="n">myAxes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

<span class="n">circle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_21_0.png" src="../../_images/single-network-models_RDPG_21_0.png" />
</div>
</div>
<p>Now what is their intersection? Remember that the intersection of two sets <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    A \cap B &amp;= \{x : x \in A, x \in B\}
\end{align*}\]</div>
<p>That is, each element must be in <em>both</em> sets to be in the intersection. The interesction of the unit ball and the non-negative orthant will be the set:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
   \mathcal X_K = \left\{\vec x \in \mathbb R^K :\left|\left|\vec x\right|\right|_2 \leq 1, x_k \geq 0 \textrm{ for all $k$}\right\}
\end{align*}\]</div>
<p>visually, this will be the set of points in the <em>overlap</em> of the unit ball and the non-negative orthant, which we show below in purple:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axes</span> <span class="o">=</span> <span class="n">myAxes</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">2.5</span><span class="p">,</span><span class="mf">2.5</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">axes</span><span class="o">.</span><span class="n">draw</span><span class="p">()</span>

<span class="n">circle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.3</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">circle</span><span class="p">)</span>
<span class="n">rectangle</span> <span class="o">=</span><span class="n">patch</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="n">ec</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.2</span><span class="p">)</span>
<span class="n">axes</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rectangle</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/single-network-models_RDPG_23_0.png" src="../../_images/single-network-models_RDPG_23_0.png" />
</div>
</div>
<p>This space has an <em>incredibly</em> important corollary. It turns out that if <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> are both elements of <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>, that <span class="math notranslate nohighlight">\(\left\langle \vec x, \vec y \right \rangle = \vec x^\top \vec y\)</span>, the <strong>inner product</strong>, is at most <span class="math notranslate nohighlight">\(1\)</span>, and at least <span class="math notranslate nohighlight">\(0\)</span>. Without getting too technical, this is because of something called the Cauchy-Schwartz inequality and the properties of <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>. If you remember from linear algebra, the Cauchy-Schwartz inequality states that <span class="math notranslate nohighlight">\(\left\langle \vec x, \vec y \right \rangle\)</span> can be at most the product of <span class="math notranslate nohighlight">\(\left|\left|\vec x\right|\right|_2\)</span> and <span class="math notranslate nohighlight">\(\left|\left|\vec y\right|\right|_2\)</span>. Since <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> have norms both less than or equal to <span class="math notranslate nohighlight">\(1\)</span> (since they are on the <em>unit ball</em>), their inner-product is at most <span class="math notranslate nohighlight">\(1\)</span>. Further, since <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> are in the non-negative orthant, their inner product can never be negative. This is because both <span class="math notranslate nohighlight">\(\vec x\)</span> and <span class="math notranslate nohighlight">\(\vec y\)</span> have entries which are not negative, and therefore their element-wise products can never be negative.</p>
<p>The <em>a posteriori</em> RDPG is to the <em>a priori</em> RDPG what the <em>a posteriori</em> SBM was to the <em>a priori</em> SBM. We instead suppose that we do <em>not</em> know the latent position matrix <span class="math notranslate nohighlight">\(X\)</span>, but instead know how we can characterize the individual latent positions. We have the following parameter:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>F</p></td>
<td><p>inner-product distributions</p></td>
<td><p>A distribution which governs each latent position.</p></td>
</tr>
</tbody>
</table>
<p>The parameter <span class="math notranslate nohighlight">\(F\)</span> is what is known as an <strong>inner-product distribution</strong>. In the simplest case, we will assume that <span class="math notranslate nohighlight">\(F\)</span> is a distribution on a subset of the possible real vectors that have <span class="math notranslate nohighlight">\(d\)</span>-dimensions with an important caveat: for any two vectors within this subset, their inner product <em>must</em> be a probability. We will refer to the subset of the possible real vectors as <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>, which we learned about above. This means that for any <span class="math notranslate nohighlight">\(\vec x_i, \vec x_j\)</span> that are in <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>, it is always the case that <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span> is between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. This is essential because like previously, we will describe the distribution of each edge in the adjacency matrix using <span class="math notranslate nohighlight">\(\vec x_i^\top \vec x_j\)</span> to represent a probability. Next, we will treat the latent position matrix as a matrix-valued random variable which is <em>latent</em> (remember, <em>latent</em> means that we don’t get to see it in our real data). Like before, we will call <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i\)</span> the random latent positions for the nodes of our network. In this case, each <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span> is sampled independently and identically from the inner-product distribution <span class="math notranslate nohighlight">\(F\)</span> described above. The latent-position matrix is the matrix-valued random variable <span class="math notranslate nohighlight">\(\mathbf X\)</span> whose entries are the latent vectors <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span>, for each of the <span class="math notranslate nohighlight">\(n\)</span> nodes.</p>
<p>The model for edges of the <em>a posteriori</em> RDPG can be described by conditioning on this unobserved latent-position matrix. We write down that, conditioned on <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i = \vec x\)</span> and <span class="math notranslate nohighlight">\(\vec {\mathbf x}_j = \vec y\)</span>, that if <span class="math notranslate nohighlight">\(j &gt; i\)</span>, then <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(\vec x^\top \vec y)\)</span> distribution. As before, if <span class="math notranslate nohighlight">\(i &lt; j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (the network is <em>undirected</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> (the network is <em>loopless</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an <em>a posteriori</em> RDPG with parameter <span class="math notranslate nohighlight">\(F\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim RDPG_n(F)\)</span>.</p>
<div class="section" id="id1">
<h3><span class="section-number">5.5.3.1. </span>Probability*<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The probability for the <em>a posteriori</em> RDPG is fairly complicated. This is because, like the <em>a posteriori</em> SBM, we do not actually get to see the latent position matrix <span class="math notranslate nohighlight">\(\mathbf X\)</span>, so we need to use <em>integration</em> to obtain an expression for the probability. Here, we are concerned with realizations of <span class="math notranslate nohighlight">\(\mathbf X\)</span>. Remember that <span class="math notranslate nohighlight">\(\mathbf X\)</span> is just a matrix whose rows are <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i\)</span>, each of which individually have have the distribution <span class="math notranslate nohighlight">\(F\)</span>; e.g., <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i \sim F\)</span> independently. For simplicity, we will assume that <span class="math notranslate nohighlight">\(F\)</span> is a disrete distribution on <span class="math notranslate nohighlight">\(\mathcal X_K\)</span>. This makes the logic of what is going on below much simpler since the notation gets less complicated, but does not detract from the generalizability of the result (the only difference is that sums would be replaced by multivariate integrals, and probability mass functions replaced by probability density functions).</p>
<p>We will let <span class="math notranslate nohighlight">\(p\)</span> denote the probability mass function (p.m.f.) of this discrete distribution function <span class="math notranslate nohighlight">\(F\)</span>. The strategy will be to use the independence assumption, followed by integration over the relevant rows of <span class="math notranslate nohighlight">\(\mathbf X\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;= \mathbb P_\theta(\mathbf A = A) \\
&amp;= \prod_{j &gt; i} \mathbb P(\mathbf a_{ij} = a_{ij}), \;\;\;\;\textrm{Independence Assumption} \\
\mathbb P(\mathbf a_{ij} = a_{ij})&amp;= \sum_{\vec x \in \mathcal X_K}\sum_{\vec y \in \mathcal X_K}\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y),\;\;\;\;\textrm{integration over }\vec {\mathbf x}_i \textrm{ and }\vec {\mathbf x}_j
\end{align*}\]</div>
<p>Next, we will simplify this expression a little bit more, using the definition of a conditional probability like we did before for the SBM:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\\
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;= \mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) \mathbb P(\vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Further, remember that if <span class="math notranslate nohighlight">\(\mathbf a\)</span> and <span class="math notranslate nohighlight">\(\mathbf b\)</span> are independent, then <span class="math notranslate nohighlight">\(\mathbb P(\mathbf a = a, \mathbf b = b) = \mathbb P(\mathbf a = a)\mathbb P(\mathbf b = b)\)</span>. Using that <span class="math notranslate nohighlight">\(\vec x_i\)</span> and <span class="math notranslate nohighlight">\(\vec x_j\)</span> are independent, by definition:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;= \mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Which means that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;=  \mathbb P(\mathbf a_{ij} = a_{ij} | \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y)\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>Finally, we that conditional on <span class="math notranslate nohighlight">\(\vec{\mathbf x}_i = \vec x_i\)</span> and <span class="math notranslate nohighlight">\(\vec{\mathbf x}_j = \vec x_j\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is <span class="math notranslate nohighlight">\(Bern(\vec x_i^\top \vec x_j)\)</span>. This means that in terms of our probability matrix, each entry <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec x_j\)</span>. Therefore:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}| \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;= (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}
\end{align*}\]</div>
<p>This implies that:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P(\mathbf a_{ij} = a_{ij}, \vec{\mathbf x}_i = \vec x, \vec{\mathbf x}_j = \vec y) &amp;=  (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
<p>So our complete expression for the probability is:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb P_\theta(A) &amp;= \prod_{j &gt; i}\sum_{\vec x \in \mathcal X_K}\sum_{\vec y \in \mathcal X_K} (\vec x^\top \vec y)^{a_{ij}}(1 - \vec x^\top\vec y)^{1 - a_{ij}}\mathbb P(\vec{\mathbf x}_i = \vec x) \mathbb P(\vec{\mathbf x}_j = \vec y)
\end{align*}\]</div>
</div>
</div>
<div class="section" id="generalized-random-dot-product-graph-grdpg">
<h2><span class="section-number">5.5.4. </span>Generalized Random Dot Product Graph (GRDPG)<a class="headerlink" href="#generalized-random-dot-product-graph-grdpg" title="Permalink to this headline">¶</a></h2>
<p>The Generalized Random Dot Product Graph, or GRDPG, is the most general random network model we will consider in this book. Note that for the RDPG, the probability matrix <span class="math notranslate nohighlight">\(P\)</span> had entries <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec x_j\)</span>. What about <span class="math notranslate nohighlight">\(p_{ji}\)</span>? Well, <span class="math notranslate nohighlight">\(p_{ji} = \vec x_j^\top \vec x_i\)</span>, which is exactly the same as <span class="math notranslate nohighlight">\(p_{ij}\)</span>! This means that even if we were to consider a directed RDPG, the probabilities that can be captured are <em>always</em> going to be symmetric. The generalized random dot product graph, or GRDPG, relaxes this assumption. This is achieved by using <em>two</em> latent positin matrices, <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, and letting <span class="math notranslate nohighlight">\(P = X Y^\top\)</span>. Now, the entries <span class="math notranslate nohighlight">\(p_{ij} = \vec x_i^\top \vec y_j\)</span>, but <span class="math notranslate nohighlight">\(p_{ji} = \vec x_j^\top \vec y_i\)</span>, which might be different.</p>
<div class="section" id="a-priori-grdpg">
<h3><span class="section-number">5.5.4.1. </span><em>A Priori</em> GRDPG<a class="headerlink" href="#a-priori-grdpg" title="Permalink to this headline">¶</a></h3>
<p>The <em>a priori</em> GRDPG is a GRDPG in which we know <em>a priori</em> the latent position matrices <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. The <em>a priori</em> GRDPG has the following parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(X\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \mathbb R^{n \times d}\)</span></p></td>
<td><p>The matrix of left latent positions for each node <span class="math notranslate nohighlight">\(n\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(Y\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\( \mathbb R^{n \times d}\)</span></p></td>
<td><p>The matrix of right latent positions for each node <span class="math notranslate nohighlight">\(n\)</span>.</p></td>
</tr>
</tbody>
</table>
<p><span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> behave nearly the same as the latent position matrix <span class="math notranslate nohighlight">\(X\)</span> for the <em>a priori</em> RDPG, with the exception that they will be called the <strong>left latent position matrix</strong> and the <strong>right latent position matrix</strong> respectively. Further, the vectors <span class="math notranslate nohighlight">\(\vec x_i\)</span> will be the left latent positions, and <span class="math notranslate nohighlight">\(\vec y_i\)</span> will be the right latent positions, for a given node <span class="math notranslate nohighlight">\(i\)</span>, for each node <span class="math notranslate nohighlight">\(i=1,...,n\)</span>.</p>
<p>What is the generative model for the <em>a priori</em> GRDPG? As we discussed above, given <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, for all <span class="math notranslate nohighlight">\(j \neq i\)</span>, <span class="math notranslate nohighlight">\(\mathbf a_{ij} \sim Bern(\vec x_i^\top \vec y_j)\)</span> independently. If we consider only loopless networks, <span class="math notranslate nohighlight">\(\mathbf a_{ij} = 0\)</span>. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is an <em>a priori</em> GRDPG with left and right latent position matrices <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim GRDPG_n(X, Y)\)</span>.</p>
</div>
<div class="section" id="a-posteriori-grdpg">
<h3><span class="section-number">5.5.4.2. </span><em>A Posteriori</em> GRDPG<a class="headerlink" href="#a-posteriori-grdpg" title="Permalink to this headline">¶</a></h3>
<p>The <em>A Posteriori</em> GRDPG is very similar to the <em>a posteriori</em> RDPG. We have two parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>F</p></td>
<td><p>inner-product distributions</p></td>
<td><p>A distribution which governs the left latent positions.</p></td>
</tr>
<tr class="row-odd"><td><p>G</p></td>
<td><p>inner-product distributions</p></td>
<td><p>A distribution which governs the right latent positions.</p></td>
</tr>
</tbody>
</table>
<p>Here, we treat the left and right latent position matrices as latent variable matrices, like we did for <em>a posteriori</em> RDPG. That is, the left latent positions are sampled independently and identically from <span class="math notranslate nohighlight">\(F\)</span>, and the right latent positions <span class="math notranslate nohighlight">\(\vec y_i\)</span> are sampled independently and identically from <span class="math notranslate nohighlight">\(G\)</span>.</p>
<p>The model for edges of the <em>a posteriori</em> RDPG can be described by conditioning on the unobserved left and right latent-position matrices. We write down that, conditioned on <span class="math notranslate nohighlight">\(\vec {\mathbf x}_i = \vec x\)</span> and <span class="math notranslate nohighlight">\(\vec {\mathbf y}_j = \vec y\)</span>, that if <span class="math notranslate nohighlight">\(j \neq i\)</span>, then <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> is sampled independently from a <span class="math notranslate nohighlight">\(Bern(\vec x^\top \vec y)\)</span> distribution. As before, assuming the network is loopless, <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span>. If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency matrix for an <em>a posteriori</em> RDPG with parameter <span class="math notranslate nohighlight">\(F\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim GRDPG_n(F, G)\)</span>.</p>
</div>
</div>
<div class="section" id="inhomogeneous-erdos-renyi-ier">
<h2><span class="section-number">5.5.5. </span>Inhomogeneous Erdös-Rényi (IER)<a class="headerlink" href="#inhomogeneous-erdos-renyi-ier" title="Permalink to this headline">¶</a></h2>
<p>In the preceding models, we typically made assumptions about how we could characterize the edge-existence probabilities using fewer than <span class="math notranslate nohighlight">\(\binom n 2\)</span> different probabilities (one for each edge). The reason for this is that in general, <span class="math notranslate nohighlight">\(n\)</span> is usually relatively large, so attempting to actually learn <span class="math notranslate nohighlight">\(\binom n 2\)</span> different probabilities is not, in general, going to be very feasible (it is <em>never</em> feasible when we have a single network, since a single network only one observation for each independent edge). Further, it is relatively difficult to ask questions for which assuming edges share <em>nothing</em> in common (even if they don’t share the same probabilities, there may be properties underlying the probabilities, such as the <em>latent positions</em> that we saw above with the RDPG, that we might still want to characterize) is actually favorable.</p>
<p>Nonetheless, the most general model for an independent-edge random network is known as the Inhomogeneous Erdös-Rényi (IER) Random Network. An IER Random Network is characterized by the following parameters:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Space</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(P\)</span></p></td>
<td><p>[0,1]<span class="math notranslate nohighlight">\(^{n \times n}\)</span></p></td>
<td><p>The edge probability matrix.</p></td>
</tr>
</tbody>
</table>
<p>The probability matrix <span class="math notranslate nohighlight">\(P\)</span> is an <span class="math notranslate nohighlight">\(n \times n\)</span> matrix, where each entry <span class="math notranslate nohighlight">\(p_{ij}\)</span> is a probability (a value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>). Further, if we restrict ourselves to the case of simple networks like we have done so far, <span class="math notranslate nohighlight">\(P\)</span> will also be symmetric (<span class="math notranslate nohighlight">\(p_{ij} = p_{ji}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span>). The generative model is similar to the preceding models we have seen: given the <span class="math notranslate nohighlight">\((i, j)\)</span> entry of <span class="math notranslate nohighlight">\(P\)</span>, denoted <span class="math notranslate nohighlight">\(p_{ij}\)</span>, the edges <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span> are independent <span class="math notranslate nohighlight">\(Bern(p_{ij})\)</span>, for any <span class="math notranslate nohighlight">\(j &gt; i\)</span>. Further, <span class="math notranslate nohighlight">\(\mathbf a_{ii} = 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> (the network is <em>loopless</em>), and <span class="math notranslate nohighlight">\(\mathbf a_{ji} = \mathbf a_{ij}\)</span> (the network is <em>undirected</em>). If <span class="math notranslate nohighlight">\(\mathbf A\)</span> is the adjacency maatrix for an IER network with probability matarix <span class="math notranslate nohighlight">\(P\)</span>, we write that <span class="math notranslate nohighlight">\(\mathbf A \sim IER_n(P)\)</span>.</p>
<p>It is worth noting that <em>all</em> of the preceding models we have discussed so far are special cases of the IER model. This means that, for instance, if we were to consider only the probability matrices where all of the entries are the same, we could represent the ER models. Similarly, if we were to only to consider the probability matrices <span class="math notranslate nohighlight">\(P\)</span> where <span class="math notranslate nohighlight">\(P = XX^\top\)</span>, we could represent any RDPG.</p>
<p>The IER Random Network can be thought of as the limit of Stochastic Block Models, as the number of communities equals the number of nodes in the network. Stated another way, an SBM Random Network where each node is in its own community is equivalent to an IER Random Network. Under this formulation, note that the block matarix for such an SBM, <span class="math notranslate nohighlight">\(B\)</span>, would have <span class="math notranslate nohighlight">\(n \times n\)</span> unique entries. Taking <span class="math notranslate nohighlight">\(P\)</span> to be this block matrix shows that the IER is a limiting case of SBMs.</p>
<div class="section" id="id2">
<h3><span class="section-number">5.5.5.1. </span>Probability*<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>The probability for a network which is IER is very straightforward. We use the independence assumption, and the p.m.f. of a Bernoulli-distributed random-variable <span class="math notranslate nohighlight">\(\mathbf a_{ij}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb P_\theta(A) &amp;= \mathbb P(\mathbf A = A) \\
    &amp;= \prod_{j &gt; i}p_{ij}^{a_{ij}}(1 - p_{ij})^{1 - a_{ij}}
\end{align*}\]</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./representations/ch5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="single-network-models_SBM.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title"><span class="section-number">5.4. </span>Stochastic Block Models (SBM)</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="multi-network-models.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title"><span class="section-number">5.6. </span>Multi-Network Models</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Joshua Vogelstein, Alex Loftus, and Eric Bridgeford<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>